{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 猫狗大战\n",
    "\n",
    "kaggle题目 [Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats)\n",
    "\n",
    "设计一种算法，区分图片中是猫还是狗。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、引入函数库\n",
    "\n",
    "- numpy: Anaconda环境自带\n",
    "- tensorflow: 自行下载，gpu版\n",
    "- cPickle: Anaconda环境自带\n",
    "- cv2: opencv2\n",
    "- datetime: Anaconda环境自带\n",
    "- sklearn.model_selection.train_test_split: Anaconda环境自带，需要高版本sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import cv2\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, train_label = pickle.load(open('data_process/train_image_label_list.p', mode='rb'))\n",
    "test = train[(len(train)-1000):]\n",
    "test_label = train_label[(len(train_label)-1000):]\n",
    "train = train[:(len(train)-1000)]\n",
    "train_label = train_label[:(len(train_label)-1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、神经网络构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 16\n",
    "CAPACITY = 2000\n",
    "IMG_W = 208\n",
    "IMG_H = 208\n",
    "EPOCH_NUMS = 10\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 神经网络网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images, batch_size, n_classes):\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [3, 3, 3, 16],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                  shape = [16],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(images, weights, strides=[1,1,1,1], padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        \n",
    "    # pool1 and norm1\n",
    "    with tf.variable_scope('pooling1_lrn') as scope:\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1], strides=[1,2,2,1], padding='SAME', name='pooling1')\n",
    "        norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='norm1')\n",
    "        \n",
    "    # conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [3, 3, 16, 16],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                  shape = [16],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(norm1, weights, strides=[1,1,1,1], padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name='conv2')\n",
    "        \n",
    "    # pool2 and norm2\n",
    "    with tf.variable_scope('pooling2_lrn') as scope:\n",
    "        norm2 = tf.nn.lrn(conv2, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='norm2')\n",
    "        pool2 = tf.nn.max_pool(norm2, ksize=[1,3,3,1], strides=[1,1,1,1], padding='SAME', name='pooling2')\n",
    "    \n",
    "    with tf.variable_scope('dropout1') as scope:\n",
    "        reshape = tf.reshape(pool2, shape=[batch_size, -1])\n",
    "        dropout_layer1 = tf.nn.dropout(reshape, dropout_keep_prob, name = \"dropout_layer1\")\n",
    "        \n",
    "    # local3\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [173056, 128],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                  shape = [128],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.constant_initializer(0.1))\n",
    "        local3 = tf.nn.relu(tf.matmul(dropout_layer1, weights) + biases, name=scope.name)\n",
    "    \n",
    "    with tf.variable_scope('dropout2') as scope:\n",
    "        dropout_layer2 = tf.nn.dropout(local3, dropout_keep_prob, name = \"dropout_layer2\")\n",
    "        \n",
    "    # local4\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [128, 128],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                  shape = [128],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.constant_initializer(0.1))\n",
    "        local4 = tf.nn.relu(tf.matmul(dropout_layer2, weights) + biases, name='local4')\n",
    "        \n",
    "    # softmax\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = tf.get_variable('softmax_linear',\n",
    "                                  shape = [128, n_classes],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                  shape = [n_classes],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.constant_initializer(0.1))\n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name='softmax_linear')\n",
    "        \n",
    "    return softmax_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    image = tf.placeholder(dtype=tf.float32, shape=[None, IMG_W, IMG_H, 3], name=\"image\")\n",
    "    label = tf.placeholder(dtype=tf.int32, shape=[None], name=\"label\")\n",
    "    LearningRate = tf.placeholder(tf.float32, name=\"LearningRate\")\n",
    "    \n",
    "    with tf.variable_scope(\"inference\"):\n",
    "        softmax_linear = inference(image, BATCH_SIZE, N_CLASSES)\n",
    "    \n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=softmax_linear, labels=label)\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "    \n",
    "    with tf.variable_scope(\"optimizer\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    with tf.variable_scope(\"accuracy\"):\n",
    "        correct = tf.nn.in_top_k(softmax_linear, label, 1)\n",
    "        correct = tf.cast(correct, tf.float16)\n",
    "        accuracy = tf.reduce_mean(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 取得batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_batches(image, label, batch_size):\n",
    "    for start in range(0, len(label), batch_size):\n",
    "        end = min(start + batch_size, len(label))\n",
    "        yield image[start:end], label[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、训练和预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = {'train':[], 'cv':[]}\n",
    "accurates = {'train':[], 'cv':[], 'test':[]}\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "    accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "    train_summary_writer = tf.summary.FileWriter(\"runs/train\", sess.graph)\n",
    "    \n",
    "    # CV Summaries\n",
    "    cv_summary_op = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "    cv_summary_writer = tf.summary.FileWriter(\"runs/cv\", sess.graph)\n",
    "    \n",
    "    # Test Summaries\n",
    "    test_summary_op = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "    test_summary_writer = tf.summary.FileWriter(\"runs/test\", sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(EPOCH_NUMS):\n",
    "        train_X, cv_X, train_y, cv_y = train_test_split(train, train_label, test_size = 0.05)\n",
    "        \n",
    "        train_batches = get_batches(train_X, train_y, BATCH_SIZE)\n",
    "        for batch_i in range(len(train_X) // BATCH_SIZE):\n",
    "            x, y = next(train_batches)\n",
    "            img = np.zeros([BATCH_SIZE, IMG_W, IMG_H, 3])\n",
    "            for i in range(BATCH_SIZE):\n",
    "                im = cv2.imread(x[i])\n",
    "                if (im != None):\n",
    "                    im = cv2.resize(im, (IMG_W, IMG_H))\n",
    "                    img[i]=im\n",
    "            \n",
    "            feed = {image: img,\n",
    "                    label: np.reshape(y, [BATCH_SIZE]),\n",
    "                    LearningRate: learning_rate}\n",
    "            step, train_loss, summaries, _, accurate = sess.run([global_step, loss, train_summary_op, train_op, accuracy], feed)  #cost\n",
    "            losses['train'].append(train_loss)\n",
    "            accurates['train'].append(accurate)\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "            \n",
    "            if (epoch_i * (len(train_X) // BATCH_SIZE) + batch_i) % 100 == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}    accuracy = {:.3f}'.format(time_str,\n",
    "                                                                                                         epoch_i,\n",
    "                                                                                                         batch_i,\n",
    "                                                                                                         (len(train_X) // BATCH_SIZE),\n",
    "                                                                                                         train_loss,\n",
    "                                                                                                         accurate))\n",
    "        cv_batches = get_batches(cv_X, cv_y, BATCH_SIZE)\n",
    "        for batch_i in range(len(cv_X) // BATCH_SIZE):\n",
    "            x, y = next(cv_batches)\n",
    "            img = np.zeros([BATCH_SIZE, IMG_W, IMG_H, 3])\n",
    "            for i in range(BATCH_SIZE):\n",
    "                im = cv2.imread(x[i])\n",
    "                if (im != None):\n",
    "                    im = cv2.resize(im, (IMG_W, IMG_H))\n",
    "                    img[i]=im\n",
    "            \n",
    "            feed = {image: img,\n",
    "                    label: np.reshape(y, [BATCH_SIZE]),\n",
    "                    LearningRate: learning_rate}\n",
    "            step, cv_loss, summaries, accurate = sess.run([global_step, loss, cv_summary_op, accuracy], feed)  #cost\n",
    "            losses['cv'].append(cv_loss)\n",
    "            accurates['cv'].append(accurate)\n",
    "            cv_summary_writer.add_summary(summaries, step)\n",
    "            \n",
    "            if (epoch_i * (len(cv_X) // BATCH_SIZE) + batch_i) % 20 == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   test_loss = {:.3f}    accuracy = {:.3f}'.format(time_str,\n",
    "                                                                                                        epoch_i,\n",
    "                                                                                                        batch_i,\n",
    "                                                                                                        (len(cv_X) // BATCH_SIZE),\n",
    "                                                                                                        cv_loss,\n",
    "                                                                                                        accurate))\n",
    "    test_batches = get_batches(test, test_label, BATCH_SIZE)\n",
    "    for batch_i in range(len(test) // BATCH_SIZE):\n",
    "        x, y = next(test_batches)\n",
    "        img = np.zeros([BATCH_SIZE, IMG_W, IMG_H, 3])\n",
    "        for i in range(BATCH_SIZE):\n",
    "            im = cv2.imread(x[i])\n",
    "            if (im != None):\n",
    "                im = cv2.resize(im, (IMG_W, IMG_H))\n",
    "                img[i]=im\n",
    "        feed = {image: img,\n",
    "                label: np.reshape(y, [BATCH_SIZE]),\n",
    "                LearningRate: learning_rate}\n",
    "        step, summaries, accurate = sess.run([global_step, test_summary_op, accuracy], feed)  #cost\n",
    "        accurates['test'].append(accurate)\n",
    "        test_summary_writer.add_summary(summaries, step)\n",
    "    print(\"accuracy = {:.3f}\".format(np.mean(accurates['test'])))\n",
    "    saver.save(sess, 'save/save')\n",
    "    print('Model Trained and Saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
