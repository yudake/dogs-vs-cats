{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 猫狗大战\n",
    "\n",
    "kaggle题目 [Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats)\n",
    "\n",
    "设计一种算法，区分图片中是猫还是狗。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、引入函数库\n",
    "\n",
    "- numpy: Anaconda环境自带\n",
    "- os： Anaconda环境自带\n",
    "- tensorflow: 自行下载，gpu版\n",
    "- cPickle: Anaconda环境自带"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import cv2\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, train_label = pickle.load(open('data_process/train_image_label_list.p', mode='rb'))\n",
    "test = train[(len(train)-1000):]\n",
    "test_label = train_label[(len(train_label)-1000):]\n",
    "train = train[:(len(train)-1000)]\n",
    "train_label = train_label[:(len(train_label)-1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、神经网络构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 16\n",
    "CAPACITY = 2000\n",
    "IMG_W = 208\n",
    "IMG_H = 208\n",
    "EPOCH_NUMS = 10\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 神经网络网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images, batch_size, n_classes, dropout_keep_prob):\n",
    "    '''Build the model\n",
    "    Args:\n",
    "        images: image batch, 4D tensor, tf.float32, [batch_size, width, height, channels]\n",
    "        \n",
    "    Returns:\n",
    "        output tensor with the computed logits, float, [batch_size, n_classes]\n",
    "    '''\n",
    "    # conv1, shape = [kernel size, kernel size, channels, kernel numbers]\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [3, 3, 3, 16],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                  shape = [16],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(images, weights, strides=[1,1,1,1], padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        \n",
    "    # pool1 and norm1\n",
    "    with tf.variable_scope('pooling1_lrn') as scope:\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1], strides=[1,2,2,1], padding='SAME', name='pooling1')\n",
    "        norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='norm1')\n",
    "        \n",
    "    # conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [3, 3, 16, 16],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                  shape = [16],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(norm1, weights, strides=[1,1,1,1], padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name='conv2')\n",
    "        \n",
    "    # pool2 and norm2\n",
    "    with tf.variable_scope('pooling2_lrn') as scope:\n",
    "        norm2 = tf.nn.lrn(conv2, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='norm2')\n",
    "        pool2 = tf.nn.max_pool(norm2, ksize=[1,3,3,1], strides=[1,1,1,1], padding='SAME', name='pooling2')\n",
    "    \n",
    "    with tf.variable_scope('dropout1') as scope:\n",
    "        reshape = tf.reshape(pool2, shape=[batch_size, -1])\n",
    "        dropout_layer1 = tf.nn.dropout(reshape, dropout_keep_prob, name = \"dropout_layer1\")\n",
    "        \n",
    "    # local3\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [173056, 128],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                  shape = [128],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.constant_initializer(0.1))\n",
    "        local3 = tf.nn.relu(tf.matmul(dropout_layer1, weights) + biases, name=scope.name)\n",
    "    \n",
    "    with tf.variable_scope('dropout2') as scope:\n",
    "        dropout_layer2 = tf.nn.dropout(local3, dropout_keep_prob, name = \"dropout_layer2\")\n",
    "        \n",
    "    # local4\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [128, 128],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                  shape = [128],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.constant_initializer(0.1))\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name='local4')\n",
    "        \n",
    "    # softmax\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = tf.get_variable('softmax_linear',\n",
    "                                  shape = [128, n_classes],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                  shape = [n_classes],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.constant_initializer(0.1))\n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name='softmax_linear')\n",
    "        \n",
    "    return softmax_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    image = tf.placeholder(dtype=tf.float32, shape=[None, IMG_W, IMG_H, 3], name=\"image\")\n",
    "    label = tf.placeholder(dtype=tf.int32, shape=[None], name=\"label\")\n",
    "    LearningRate = tf.placeholder(tf.float32, name=\"LearningRate\")\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "    \n",
    "    with tf.variable_scope(\"inference\"):\n",
    "        softmax_linear = inference(image, BATCH_SIZE, N_CLASSES, dropout_keep_prob)\n",
    "    \n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=softmax_linear, labels=label)\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "    \n",
    "    with tf.variable_scope(\"optimizer\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    with tf.variable_scope(\"accuracy\"):\n",
    "        correct = tf.nn.in_top_k(softmax_linear, label, 1)\n",
    "        correct = tf.cast(correct, tf.float16)\n",
    "        accuracy = tf.reduce_mean(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 取得batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_batches(image, label, batch_size):\n",
    "    for start in range(0, len(label), batch_size):\n",
    "        end = min(start + batch_size, len(label))\n",
    "        yield image[start:end], label[start:end]\n",
    "\n",
    "def get_batches_test(image, batch_size):\n",
    "    for start in range(0, len(image), batch_size):\n",
    "        end = min(start + batch_size, len(image))\n",
    "        yield image[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、训练和预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yudake/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:23: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-25T14:38:02.835598: Epoch   0 Batch    0/1425   train_loss = 0.692    accuracy = 0.625\n",
      "2018-02-25T14:38:13.887219: Epoch   0 Batch   20/1425   train_loss = 0.694    accuracy = 0.500\n",
      "2018-02-25T14:38:25.004065: Epoch   0 Batch   40/1425   train_loss = 0.701    accuracy = 0.312\n",
      "2018-02-25T14:38:36.075831: Epoch   0 Batch   60/1425   train_loss = 0.688    accuracy = 0.625\n",
      "2018-02-25T14:38:47.163976: Epoch   0 Batch   80/1425   train_loss = 0.693    accuracy = 0.625\n",
      "2018-02-25T14:38:58.253145: Epoch   0 Batch  100/1425   train_loss = 0.698    accuracy = 0.375\n",
      "2018-02-25T14:39:09.333548: Epoch   0 Batch  120/1425   train_loss = 0.685    accuracy = 0.562\n",
      "2018-02-25T14:39:20.394427: Epoch   0 Batch  140/1425   train_loss = 0.698    accuracy = 0.250\n",
      "2018-02-25T14:39:31.487539: Epoch   0 Batch  160/1425   train_loss = 0.731    accuracy = 0.375\n",
      "2018-02-25T14:39:42.581483: Epoch   0 Batch  180/1425   train_loss = 0.623    accuracy = 0.750\n",
      "2018-02-25T14:39:53.671515: Epoch   0 Batch  200/1425   train_loss = 0.667    accuracy = 0.562\n",
      "2018-02-25T14:40:04.796907: Epoch   0 Batch  220/1425   train_loss = 0.711    accuracy = 0.438\n",
      "2018-02-25T14:40:15.870423: Epoch   0 Batch  240/1425   train_loss = 0.679    accuracy = 0.500\n",
      "2018-02-25T14:40:27.002860: Epoch   0 Batch  260/1425   train_loss = 0.705    accuracy = 0.500\n",
      "2018-02-25T14:40:38.113837: Epoch   0 Batch  280/1425   train_loss = 0.657    accuracy = 0.562\n",
      "2018-02-25T14:40:49.250458: Epoch   0 Batch  300/1425   train_loss = 0.692    accuracy = 0.562\n",
      "2018-02-25T14:41:00.344533: Epoch   0 Batch  320/1425   train_loss = 0.599    accuracy = 0.625\n",
      "2018-02-25T14:41:11.458469: Epoch   0 Batch  340/1425   train_loss = 0.713    accuracy = 0.562\n",
      "2018-02-25T14:41:22.545876: Epoch   0 Batch  360/1425   train_loss = 0.646    accuracy = 0.562\n",
      "2018-02-25T14:41:33.657579: Epoch   0 Batch  380/1425   train_loss = 0.644    accuracy = 0.875\n",
      "2018-02-25T14:41:44.778942: Epoch   0 Batch  400/1425   train_loss = 0.561    accuracy = 0.750\n",
      "2018-02-25T14:41:55.935791: Epoch   0 Batch  420/1425   train_loss = 0.578    accuracy = 0.562\n",
      "2018-02-25T14:42:07.009718: Epoch   0 Batch  440/1425   train_loss = 0.626    accuracy = 0.688\n",
      "2018-02-25T14:42:18.079877: Epoch   0 Batch  460/1425   train_loss = 0.909    accuracy = 0.625\n",
      "2018-02-25T14:42:29.206419: Epoch   0 Batch  480/1425   train_loss = 0.597    accuracy = 0.562\n",
      "2018-02-25T14:42:40.327457: Epoch   0 Batch  500/1425   train_loss = 0.472    accuracy = 0.875\n",
      "2018-02-25T14:42:51.452438: Epoch   0 Batch  520/1425   train_loss = 0.685    accuracy = 0.438\n",
      "2018-02-25T14:43:03.031769: Epoch   0 Batch  540/1425   train_loss = 0.581    accuracy = 0.750\n",
      "2018-02-25T14:43:14.571487: Epoch   0 Batch  560/1425   train_loss = 0.566    accuracy = 0.688\n",
      "2018-02-25T14:43:25.683716: Epoch   0 Batch  580/1425   train_loss = 0.758    accuracy = 0.625\n",
      "2018-02-25T14:43:36.814014: Epoch   0 Batch  600/1425   train_loss = 0.480    accuracy = 0.875\n",
      "2018-02-25T14:43:47.910955: Epoch   0 Batch  620/1425   train_loss = 0.686    accuracy = 0.562\n",
      "2018-02-25T14:43:59.017533: Epoch   0 Batch  640/1425   train_loss = 0.484    accuracy = 0.812\n",
      "2018-02-25T14:44:10.157175: Epoch   0 Batch  660/1425   train_loss = 0.609    accuracy = 0.625\n",
      "2018-02-25T14:44:21.292709: Epoch   0 Batch  680/1425   train_loss = 0.453    accuracy = 0.875\n",
      "2018-02-25T14:44:32.425410: Epoch   0 Batch  700/1425   train_loss = 0.446    accuracy = 0.750\n",
      "2018-02-25T14:44:43.584551: Epoch   0 Batch  720/1425   train_loss = 0.727    accuracy = 0.562\n",
      "2018-02-25T14:44:54.745211: Epoch   0 Batch  740/1425   train_loss = 0.629    accuracy = 0.500\n",
      "2018-02-25T14:45:05.883380: Epoch   0 Batch  760/1425   train_loss = 0.500    accuracy = 0.812\n",
      "2018-02-25T14:45:17.055433: Epoch   0 Batch  780/1425   train_loss = 0.712    accuracy = 0.625\n",
      "2018-02-25T14:45:28.144385: Epoch   0 Batch  800/1425   train_loss = 0.378    accuracy = 0.875\n",
      "2018-02-25T14:45:39.316442: Epoch   0 Batch  820/1425   train_loss = 0.499    accuracy = 0.812\n",
      "2018-02-25T14:45:50.467181: Epoch   0 Batch  840/1425   train_loss = 0.531    accuracy = 0.688\n",
      "2018-02-25T14:46:01.559613: Epoch   0 Batch  860/1425   train_loss = 0.604    accuracy = 0.625\n",
      "2018-02-25T14:46:12.646992: Epoch   0 Batch  880/1425   train_loss = 0.540    accuracy = 0.812\n",
      "2018-02-25T14:46:23.782954: Epoch   0 Batch  900/1425   train_loss = 0.482    accuracy = 0.750\n",
      "2018-02-25T14:46:34.861536: Epoch   0 Batch  920/1425   train_loss = 0.606    accuracy = 0.625\n",
      "2018-02-25T14:46:45.967768: Epoch   0 Batch  940/1425   train_loss = 0.777    accuracy = 0.438\n",
      "2018-02-25T14:46:57.100892: Epoch   0 Batch  960/1425   train_loss = 0.565    accuracy = 0.562\n",
      "2018-02-25T14:47:08.214312: Epoch   0 Batch  980/1425   train_loss = 0.649    accuracy = 0.688\n",
      "2018-02-25T14:47:19.365816: Epoch   0 Batch 1000/1425   train_loss = 0.591    accuracy = 0.750\n",
      "2018-02-25T14:47:30.637108: Epoch   0 Batch 1020/1425   train_loss = 0.500    accuracy = 0.812\n",
      "2018-02-25T14:47:41.863371: Epoch   0 Batch 1040/1425   train_loss = 0.420    accuracy = 0.750\n",
      "2018-02-25T14:47:52.957609: Epoch   0 Batch 1060/1425   train_loss = 0.661    accuracy = 0.625\n",
      "2018-02-25T14:48:04.136226: Epoch   0 Batch 1080/1425   train_loss = 0.532    accuracy = 0.688\n",
      "2018-02-25T14:48:15.812691: Epoch   0 Batch 1100/1425   train_loss = 0.369    accuracy = 0.875\n",
      "2018-02-25T14:48:27.034579: Epoch   0 Batch 1120/1425   train_loss = 0.457    accuracy = 0.875\n",
      "2018-02-25T14:48:38.214535: Epoch   0 Batch 1140/1425   train_loss = 0.668    accuracy = 0.625\n",
      "2018-02-25T14:48:49.337426: Epoch   0 Batch 1160/1425   train_loss = 0.303    accuracy = 1.000\n",
      "2018-02-25T14:49:00.449613: Epoch   0 Batch 1180/1425   train_loss = 0.498    accuracy = 0.875\n",
      "2018-02-25T14:49:11.590328: Epoch   0 Batch 1200/1425   train_loss = 0.586    accuracy = 0.750\n",
      "2018-02-25T14:49:22.671771: Epoch   0 Batch 1220/1425   train_loss = 0.558    accuracy = 0.750\n",
      "2018-02-25T14:49:33.797651: Epoch   0 Batch 1240/1425   train_loss = 0.382    accuracy = 0.812\n",
      "2018-02-25T14:49:44.947129: Epoch   0 Batch 1260/1425   train_loss = 0.588    accuracy = 0.750\n",
      "2018-02-25T14:49:56.116204: Epoch   0 Batch 1280/1425   train_loss = 0.627    accuracy = 0.562\n",
      "2018-02-25T14:50:07.283671: Epoch   0 Batch 1300/1425   train_loss = 0.390    accuracy = 0.812\n",
      "2018-02-25T14:50:18.419859: Epoch   0 Batch 1320/1425   train_loss = 0.458    accuracy = 0.688\n",
      "2018-02-25T14:50:29.583883: Epoch   0 Batch 1340/1425   train_loss = 0.421    accuracy = 0.875\n",
      "2018-02-25T14:50:40.725200: Epoch   0 Batch 1360/1425   train_loss = 0.448    accuracy = 0.688\n",
      "2018-02-25T14:50:51.844641: Epoch   0 Batch 1380/1425   train_loss = 0.567    accuracy = 0.688\n",
      "2018-02-25T14:51:02.989275: Epoch   0 Batch 1400/1425   train_loss = 0.468    accuracy = 0.750\n",
      "2018-02-25T14:51:14.133189: Epoch   0 Batch 1420/1425   train_loss = 0.589    accuracy = 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yudake/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:50: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-25T14:51:16.574814: Epoch   0 Batch    0/75   test_loss = 0.596    accuracy = 0.688\n",
      "2018-02-25T14:51:20.312431: Epoch   0 Batch   20/75   test_loss = 0.493    accuracy = 0.812\n",
      "2018-02-25T14:51:24.071820: Epoch   0 Batch   40/75   test_loss = 0.441    accuracy = 0.938\n",
      "2018-02-25T14:51:27.796499: Epoch   0 Batch   60/75   test_loss = 0.614    accuracy = 0.750\n",
      "2018-02-25T14:51:39.310625: Epoch   1 Batch   15/1425   train_loss = 0.476    accuracy = 0.750\n",
      "2018-02-25T14:51:50.390571: Epoch   1 Batch   35/1425   train_loss = 0.901    accuracy = 0.375\n",
      "2018-02-25T14:52:01.471360: Epoch   1 Batch   55/1425   train_loss = 0.352    accuracy = 0.875\n",
      "2018-02-25T14:52:12.543519: Epoch   1 Batch   75/1425   train_loss = 0.270    accuracy = 0.875\n",
      "2018-02-25T14:52:23.652557: Epoch   1 Batch   95/1425   train_loss = 0.434    accuracy = 0.750\n",
      "2018-02-25T14:52:34.743731: Epoch   1 Batch  115/1425   train_loss = 0.402    accuracy = 0.812\n",
      "2018-02-25T14:52:45.831591: Epoch   1 Batch  135/1425   train_loss = 0.559    accuracy = 0.688\n",
      "2018-02-25T14:52:56.915113: Epoch   1 Batch  155/1425   train_loss = 0.391    accuracy = 0.750\n",
      "2018-02-25T14:53:08.000591: Epoch   1 Batch  175/1425   train_loss = 0.615    accuracy = 0.500\n",
      "2018-02-25T14:53:19.090607: Epoch   1 Batch  195/1425   train_loss = 0.539    accuracy = 0.750\n",
      "2018-02-25T14:53:30.167642: Epoch   1 Batch  215/1425   train_loss = 0.374    accuracy = 0.875\n",
      "2018-02-25T14:53:42.299762: Epoch   1 Batch  235/1425   train_loss = 0.359    accuracy = 0.875\n",
      "2018-02-25T14:53:53.376089: Epoch   1 Batch  255/1425   train_loss = 0.482    accuracy = 0.750\n",
      "2018-02-25T14:54:04.444230: Epoch   1 Batch  275/1425   train_loss = 0.395    accuracy = 0.812\n",
      "2018-02-25T14:54:15.550200: Epoch   1 Batch  295/1425   train_loss = 0.499    accuracy = 0.812\n",
      "2018-02-25T14:54:26.657796: Epoch   1 Batch  315/1425   train_loss = 0.541    accuracy = 0.688\n",
      "2018-02-25T14:54:37.737191: Epoch   1 Batch  335/1425   train_loss = 0.312    accuracy = 0.875\n",
      "2018-02-25T14:54:48.891816: Epoch   1 Batch  355/1425   train_loss = 0.517    accuracy = 0.875\n",
      "2018-02-25T14:55:00.161369: Epoch   1 Batch  375/1425   train_loss = 0.477    accuracy = 0.812\n",
      "2018-02-25T14:55:11.289308: Epoch   1 Batch  395/1425   train_loss = 0.614    accuracy = 0.750\n",
      "2018-02-25T14:55:22.354093: Epoch   1 Batch  415/1425   train_loss = 0.469    accuracy = 0.688\n",
      "2018-02-25T14:55:33.574460: Epoch   1 Batch  435/1425   train_loss = 0.557    accuracy = 0.750\n",
      "2018-02-25T14:55:44.758078: Epoch   1 Batch  455/1425   train_loss = 0.366    accuracy = 0.875\n",
      "2018-02-25T14:55:55.998215: Epoch   1 Batch  475/1425   train_loss = 0.438    accuracy = 0.812\n",
      "2018-02-25T14:56:07.116101: Epoch   1 Batch  495/1425   train_loss = 0.305    accuracy = 0.938\n",
      "2018-02-25T14:56:18.319274: Epoch   1 Batch  515/1425   train_loss = 0.450    accuracy = 0.812\n",
      "2018-02-25T14:56:29.394295: Epoch   1 Batch  535/1425   train_loss = 0.583    accuracy = 0.562\n",
      "2018-02-25T14:56:40.468180: Epoch   1 Batch  555/1425   train_loss = 0.571    accuracy = 0.625\n",
      "2018-02-25T14:56:51.569473: Epoch   1 Batch  575/1425   train_loss = 0.313    accuracy = 0.875\n",
      "2018-02-25T14:57:02.626912: Epoch   1 Batch  595/1425   train_loss = 0.390    accuracy = 0.812\n",
      "2018-02-25T14:57:13.759228: Epoch   1 Batch  615/1425   train_loss = 0.371    accuracy = 0.875\n",
      "2018-02-25T14:57:24.871215: Epoch   1 Batch  635/1425   train_loss = 0.744    accuracy = 0.812\n",
      "2018-02-25T14:57:35.977920: Epoch   1 Batch  655/1425   train_loss = 0.757    accuracy = 0.562\n",
      "2018-02-25T14:57:47.118415: Epoch   1 Batch  675/1425   train_loss = 0.504    accuracy = 0.812\n",
      "2018-02-25T14:57:58.216476: Epoch   1 Batch  695/1425   train_loss = 0.373    accuracy = 0.875\n",
      "2018-02-25T14:58:09.324229: Epoch   1 Batch  715/1425   train_loss = 0.544    accuracy = 0.750\n",
      "2018-02-25T14:58:20.425123: Epoch   1 Batch  735/1425   train_loss = 0.570    accuracy = 0.625\n",
      "2018-02-25T14:58:31.545262: Epoch   1 Batch  755/1425   train_loss = 0.452    accuracy = 0.750\n",
      "2018-02-25T14:58:42.640288: Epoch   1 Batch  775/1425   train_loss = 0.547    accuracy = 0.688\n",
      "2018-02-25T14:58:53.728279: Epoch   1 Batch  795/1425   train_loss = 0.566    accuracy = 0.812\n",
      "2018-02-25T14:59:04.860782: Epoch   1 Batch  815/1425   train_loss = 0.253    accuracy = 0.938\n",
      "2018-02-25T14:59:15.985432: Epoch   1 Batch  835/1425   train_loss = 0.433    accuracy = 0.812\n",
      "2018-02-25T14:59:27.079970: Epoch   1 Batch  855/1425   train_loss = 0.520    accuracy = 0.688\n",
      "2018-02-25T14:59:38.178413: Epoch   1 Batch  875/1425   train_loss = 0.383    accuracy = 0.812\n",
      "2018-02-25T14:59:49.234804: Epoch   1 Batch  895/1425   train_loss = 0.340    accuracy = 0.875\n",
      "2018-02-25T15:00:00.325653: Epoch   1 Batch  915/1425   train_loss = 0.327    accuracy = 0.875\n",
      "2018-02-25T15:00:11.390160: Epoch   1 Batch  935/1425   train_loss = 0.442    accuracy = 0.875\n",
      "2018-02-25T15:00:22.506812: Epoch   1 Batch  955/1425   train_loss = 0.584    accuracy = 0.688\n",
      "2018-02-25T15:00:33.573113: Epoch   1 Batch  975/1425   train_loss = 0.587    accuracy = 0.750\n",
      "2018-02-25T15:00:44.648764: Epoch   1 Batch  995/1425   train_loss = 0.661    accuracy = 0.625\n",
      "2018-02-25T15:00:55.794801: Epoch   1 Batch 1015/1425   train_loss = 0.643    accuracy = 0.625\n",
      "2018-02-25T15:01:06.897027: Epoch   1 Batch 1035/1425   train_loss = 0.389    accuracy = 0.875\n",
      "2018-02-25T15:01:18.481092: Epoch   1 Batch 1055/1425   train_loss = 0.654    accuracy = 0.875\n",
      "2018-02-25T15:01:30.226934: Epoch   1 Batch 1075/1425   train_loss = 0.266    accuracy = 0.938\n",
      "2018-02-25T15:01:41.342488: Epoch   1 Batch 1095/1425   train_loss = 0.407    accuracy = 0.875\n",
      "2018-02-25T15:01:52.508361: Epoch   1 Batch 1115/1425   train_loss = 0.592    accuracy = 0.688\n",
      "2018-02-25T15:02:03.650868: Epoch   1 Batch 1135/1425   train_loss = 0.485    accuracy = 0.750\n",
      "2018-02-25T15:02:14.909569: Epoch   1 Batch 1155/1425   train_loss = 0.804    accuracy = 0.625\n",
      "2018-02-25T15:02:25.997545: Epoch   1 Batch 1175/1425   train_loss = 0.521    accuracy = 0.625\n",
      "2018-02-25T15:02:37.105658: Epoch   1 Batch 1195/1425   train_loss = 0.412    accuracy = 0.875\n",
      "2018-02-25T15:02:48.216116: Epoch   1 Batch 1215/1425   train_loss = 0.716    accuracy = 0.500\n",
      "2018-02-25T15:02:59.322437: Epoch   1 Batch 1235/1425   train_loss = 0.460    accuracy = 0.688\n",
      "2018-02-25T15:03:10.427865: Epoch   1 Batch 1255/1425   train_loss = 0.438    accuracy = 0.812\n",
      "2018-02-25T15:03:21.496558: Epoch   1 Batch 1275/1425   train_loss = 0.384    accuracy = 0.750\n",
      "2018-02-25T15:03:32.591756: Epoch   1 Batch 1295/1425   train_loss = 0.551    accuracy = 0.750\n",
      "2018-02-25T15:03:43.740221: Epoch   1 Batch 1315/1425   train_loss = 0.358    accuracy = 0.812\n",
      "2018-02-25T15:03:54.822356: Epoch   1 Batch 1335/1425   train_loss = 0.371    accuracy = 0.812\n",
      "2018-02-25T15:04:05.946300: Epoch   1 Batch 1355/1425   train_loss = 0.229    accuracy = 0.938\n",
      "2018-02-25T15:04:17.065157: Epoch   1 Batch 1375/1425   train_loss = 0.308    accuracy = 0.938\n",
      "2018-02-25T15:04:28.173714: Epoch   1 Batch 1395/1425   train_loss = 0.263    accuracy = 0.938\n",
      "2018-02-25T15:04:39.242928: Epoch   1 Batch 1415/1425   train_loss = 0.282    accuracy = 0.938\n",
      "2018-02-25T15:04:45.350344: Epoch   1 Batch    5/75   test_loss = 0.401    accuracy = 0.750\n",
      "2018-02-25T15:04:49.070818: Epoch   1 Batch   25/75   test_loss = 0.572    accuracy = 0.750\n",
      "2018-02-25T15:04:52.792363: Epoch   1 Batch   45/75   test_loss = 0.607    accuracy = 0.750\n",
      "2018-02-25T15:04:56.492667: Epoch   1 Batch   65/75   test_loss = 0.337    accuracy = 0.875\n",
      "2018-02-25T15:05:04.180007: Epoch   2 Batch   10/1425   train_loss = 0.431    accuracy = 0.875\n",
      "2018-02-25T15:05:15.046002: Epoch   2 Batch   30/1425   train_loss = 0.297    accuracy = 0.875\n",
      "2018-02-25T15:05:25.943605: Epoch   2 Batch   50/1425   train_loss = 0.224    accuracy = 0.938\n",
      "2018-02-25T15:05:36.773268: Epoch   2 Batch   70/1425   train_loss = 0.471    accuracy = 0.812\n",
      "2018-02-25T15:05:47.644583: Epoch   2 Batch   90/1425   train_loss = 0.247    accuracy = 0.938\n",
      "2018-02-25T15:05:58.517308: Epoch   2 Batch  110/1425   train_loss = 0.219    accuracy = 0.875\n",
      "2018-02-25T15:06:09.394311: Epoch   2 Batch  130/1425   train_loss = 0.379    accuracy = 0.812\n",
      "2018-02-25T15:06:20.247290: Epoch   2 Batch  150/1425   train_loss = 0.594    accuracy = 0.688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-25T15:06:31.194332: Epoch   2 Batch  170/1425   train_loss = 0.602    accuracy = 0.688\n",
      "2018-02-25T15:06:42.027802: Epoch   2 Batch  190/1425   train_loss = 0.365    accuracy = 0.812\n",
      "2018-02-25T15:06:52.910171: Epoch   2 Batch  210/1425   train_loss = 0.690    accuracy = 0.688\n",
      "2018-02-25T15:07:03.748217: Epoch   2 Batch  230/1425   train_loss = 0.315    accuracy = 0.875\n",
      "2018-02-25T15:07:14.591894: Epoch   2 Batch  250/1425   train_loss = 0.543    accuracy = 0.625\n",
      "2018-02-25T15:07:25.449001: Epoch   2 Batch  270/1425   train_loss = 0.191    accuracy = 0.938\n",
      "2018-02-25T15:07:36.406546: Epoch   2 Batch  290/1425   train_loss = 0.250    accuracy = 0.938\n",
      "2018-02-25T15:07:47.926878: Epoch   2 Batch  310/1425   train_loss = 0.769    accuracy = 0.625\n",
      "2018-02-25T15:07:58.789639: Epoch   2 Batch  330/1425   train_loss = 0.695    accuracy = 0.688\n",
      "2018-02-25T15:08:09.651048: Epoch   2 Batch  350/1425   train_loss = 0.331    accuracy = 0.812\n",
      "2018-02-25T15:08:20.480760: Epoch   2 Batch  370/1425   train_loss = 0.401    accuracy = 0.812\n",
      "2018-02-25T15:08:31.343347: Epoch   2 Batch  390/1425   train_loss = 0.240    accuracy = 0.938\n",
      "2018-02-25T15:08:42.169740: Epoch   2 Batch  410/1425   train_loss = 0.327    accuracy = 0.812\n",
      "2018-02-25T15:08:53.029525: Epoch   2 Batch  430/1425   train_loss = 0.688    accuracy = 0.688\n",
      "2018-02-25T15:09:03.889783: Epoch   2 Batch  450/1425   train_loss = 0.554    accuracy = 0.875\n",
      "2018-02-25T15:09:14.788106: Epoch   2 Batch  470/1425   train_loss = 0.507    accuracy = 0.688\n",
      "2018-02-25T15:09:25.647468: Epoch   2 Batch  490/1425   train_loss = 0.469    accuracy = 0.812\n",
      "2018-02-25T15:09:36.512307: Epoch   2 Batch  510/1425   train_loss = 0.574    accuracy = 0.750\n",
      "2018-02-25T15:09:47.374098: Epoch   2 Batch  530/1425   train_loss = 0.318    accuracy = 0.812\n",
      "2018-02-25T15:09:58.218846: Epoch   2 Batch  550/1425   train_loss = 0.534    accuracy = 0.750\n",
      "2018-02-25T15:10:09.089279: Epoch   2 Batch  570/1425   train_loss = 0.266    accuracy = 0.875\n",
      "2018-02-25T15:10:19.963378: Epoch   2 Batch  590/1425   train_loss = 0.349    accuracy = 0.812\n",
      "2018-02-25T15:10:30.801659: Epoch   2 Batch  610/1425   train_loss = 0.393    accuracy = 0.812\n",
      "2018-02-25T15:10:41.691913: Epoch   2 Batch  630/1425   train_loss = 0.733    accuracy = 0.625\n",
      "2018-02-25T15:10:52.559618: Epoch   2 Batch  650/1425   train_loss = 0.301    accuracy = 0.938\n",
      "2018-02-25T15:11:03.415934: Epoch   2 Batch  670/1425   train_loss = 0.304    accuracy = 0.750\n",
      "2018-02-25T15:11:14.304138: Epoch   2 Batch  690/1425   train_loss = 0.402    accuracy = 0.750\n",
      "2018-02-25T15:11:25.132426: Epoch   2 Batch  710/1425   train_loss = 0.291    accuracy = 0.875\n",
      "2018-02-25T15:11:36.044814: Epoch   2 Batch  730/1425   train_loss = 0.459    accuracy = 0.875\n",
      "2018-02-25T15:11:46.944418: Epoch   2 Batch  750/1425   train_loss = 0.506    accuracy = 0.688\n",
      "2018-02-25T15:11:57.850628: Epoch   2 Batch  770/1425   train_loss = 0.456    accuracy = 0.750\n",
      "2018-02-25T15:12:08.698987: Epoch   2 Batch  790/1425   train_loss = 0.493    accuracy = 0.750\n",
      "2018-02-25T15:12:19.520904: Epoch   2 Batch  810/1425   train_loss = 0.374    accuracy = 0.750\n",
      "2018-02-25T15:12:30.389326: Epoch   2 Batch  830/1425   train_loss = 0.499    accuracy = 0.812\n",
      "2018-02-25T15:12:41.234329: Epoch   2 Batch  850/1425   train_loss = 0.287    accuracy = 0.812\n",
      "2018-02-25T15:12:52.567867: Epoch   2 Batch  870/1425   train_loss = 0.261    accuracy = 0.938\n",
      "2018-02-25T15:13:03.982497: Epoch   2 Batch  890/1425   train_loss = 0.414    accuracy = 0.812\n",
      "2018-02-25T15:13:14.846748: Epoch   2 Batch  910/1425   train_loss = 0.339    accuracy = 0.812\n",
      "2018-02-25T15:13:25.715117: Epoch   2 Batch  930/1425   train_loss = 0.306    accuracy = 0.938\n",
      "2018-02-25T15:13:36.564710: Epoch   2 Batch  950/1425   train_loss = 0.391    accuracy = 0.812\n",
      "2018-02-25T15:13:47.422976: Epoch   2 Batch  970/1425   train_loss = 0.582    accuracy = 0.750\n",
      "2018-02-25T15:13:58.284049: Epoch   2 Batch  990/1425   train_loss = 0.260    accuracy = 0.938\n",
      "2018-02-25T15:14:09.142833: Epoch   2 Batch 1010/1425   train_loss = 0.426    accuracy = 0.750\n",
      "2018-02-25T15:14:19.987938: Epoch   2 Batch 1030/1425   train_loss = 0.455    accuracy = 0.688\n",
      "2018-02-25T15:14:30.850917: Epoch   2 Batch 1050/1425   train_loss = 0.453    accuracy = 0.750\n",
      "2018-02-25T15:14:41.745017: Epoch   2 Batch 1070/1425   train_loss = 0.443    accuracy = 0.750\n",
      "2018-02-25T15:14:52.578878: Epoch   2 Batch 1090/1425   train_loss = 0.253    accuracy = 0.875\n",
      "2018-02-25T15:15:03.473706: Epoch   2 Batch 1110/1425   train_loss = 0.581    accuracy = 0.562\n",
      "2018-02-25T15:15:14.349895: Epoch   2 Batch 1130/1425   train_loss = 0.404    accuracy = 0.812\n",
      "2018-02-25T15:15:25.265918: Epoch   2 Batch 1150/1425   train_loss = 0.296    accuracy = 0.812\n",
      "2018-02-25T15:15:36.147686: Epoch   2 Batch 1170/1425   train_loss = 0.300    accuracy = 0.875\n",
      "2018-02-25T15:15:47.029273: Epoch   2 Batch 1190/1425   train_loss = 0.176    accuracy = 1.000\n",
      "2018-02-25T15:15:57.907289: Epoch   2 Batch 1210/1425   train_loss = 0.497    accuracy = 0.812\n",
      "2018-02-25T15:16:08.731047: Epoch   2 Batch 1230/1425   train_loss = 0.405    accuracy = 0.875\n",
      "2018-02-25T15:16:19.580426: Epoch   2 Batch 1250/1425   train_loss = 0.231    accuracy = 0.875\n",
      "2018-02-25T15:16:30.444729: Epoch   2 Batch 1270/1425   train_loss = 0.592    accuracy = 0.688\n",
      "2018-02-25T15:16:41.285300: Epoch   2 Batch 1290/1425   train_loss = 0.647    accuracy = 0.625\n",
      "2018-02-25T15:16:52.137524: Epoch   2 Batch 1310/1425   train_loss = 0.545    accuracy = 0.750\n",
      "2018-02-25T15:17:02.984279: Epoch   2 Batch 1330/1425   train_loss = 0.339    accuracy = 0.812\n",
      "2018-02-25T15:17:13.853327: Epoch   2 Batch 1350/1425   train_loss = 0.260    accuracy = 0.938\n",
      "2018-02-25T15:17:24.696640: Epoch   2 Batch 1370/1425   train_loss = 0.610    accuracy = 0.625\n",
      "2018-02-25T15:17:35.593485: Epoch   2 Batch 1390/1425   train_loss = 0.395    accuracy = 0.875\n",
      "2018-02-25T15:17:46.464949: Epoch   2 Batch 1410/1425   train_loss = 0.558    accuracy = 0.688\n",
      "2018-02-25T15:17:56.076511: Epoch   2 Batch   10/75   test_loss = 0.369    accuracy = 0.938\n",
      "2018-02-25T15:17:59.735252: Epoch   2 Batch   30/75   test_loss = 0.436    accuracy = 0.875\n",
      "2018-02-25T15:18:03.368920: Epoch   2 Batch   50/75   test_loss = 0.620    accuracy = 0.625\n",
      "2018-02-25T15:18:06.994517: Epoch   2 Batch   70/75   test_loss = 0.565    accuracy = 0.750\n",
      "2018-02-25T15:18:10.994284: Epoch   3 Batch    5/1425   train_loss = 0.270    accuracy = 0.875\n",
      "2018-02-25T15:18:21.894673: Epoch   3 Batch   25/1425   train_loss = 0.460    accuracy = 0.750\n",
      "2018-02-25T15:18:32.756275: Epoch   3 Batch   45/1425   train_loss = 0.162    accuracy = 1.000\n",
      "2018-02-25T15:18:43.589422: Epoch   3 Batch   65/1425   train_loss = 0.328    accuracy = 0.875\n",
      "2018-02-25T15:18:54.482301: Epoch   3 Batch   85/1425   train_loss = 0.310    accuracy = 0.875\n",
      "2018-02-25T15:19:05.311848: Epoch   3 Batch  105/1425   train_loss = 0.205    accuracy = 0.938\n",
      "2018-02-25T15:19:16.197047: Epoch   3 Batch  125/1425   train_loss = 0.346    accuracy = 0.812\n",
      "2018-02-25T15:19:27.041788: Epoch   3 Batch  145/1425   train_loss = 0.465    accuracy = 0.750\n",
      "2018-02-25T15:19:37.920713: Epoch   3 Batch  165/1425   train_loss = 0.307    accuracy = 0.875\n",
      "2018-02-25T15:19:48.793757: Epoch   3 Batch  185/1425   train_loss = 0.327    accuracy = 0.875\n",
      "2018-02-25T15:19:59.670961: Epoch   3 Batch  205/1425   train_loss = 0.520    accuracy = 0.688\n",
      "2018-02-25T15:20:10.527398: Epoch   3 Batch  225/1425   train_loss = 0.354    accuracy = 0.812\n",
      "2018-02-25T15:20:21.397355: Epoch   3 Batch  245/1425   train_loss = 0.272    accuracy = 0.812\n",
      "2018-02-25T15:20:32.299132: Epoch   3 Batch  265/1425   train_loss = 0.188    accuracy = 0.938\n",
      "2018-02-25T15:20:43.179882: Epoch   3 Batch  285/1425   train_loss = 0.580    accuracy = 0.812\n",
      "2018-02-25T15:20:54.355821: Epoch   3 Batch  305/1425   train_loss = 0.416    accuracy = 0.750\n",
      "2018-02-25T15:21:05.531531: Epoch   3 Batch  325/1425   train_loss = 0.337    accuracy = 0.812\n",
      "2018-02-25T15:21:16.437263: Epoch   3 Batch  345/1425   train_loss = 0.586    accuracy = 0.688\n",
      "2018-02-25T15:21:27.256138: Epoch   3 Batch  365/1425   train_loss = 0.426    accuracy = 0.688\n",
      "2018-02-25T15:21:38.123735: Epoch   3 Batch  385/1425   train_loss = 0.347    accuracy = 0.812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-25T15:21:48.978503: Epoch   3 Batch  405/1425   train_loss = 0.599    accuracy = 0.688\n",
      "2018-02-25T15:21:59.820028: Epoch   3 Batch  425/1425   train_loss = 0.266    accuracy = 0.875\n",
      "2018-02-25T15:22:10.677931: Epoch   3 Batch  445/1425   train_loss = 0.301    accuracy = 0.875\n",
      "2018-02-25T15:22:21.506130: Epoch   3 Batch  465/1425   train_loss = 0.314    accuracy = 0.938\n",
      "2018-02-25T15:22:32.377676: Epoch   3 Batch  485/1425   train_loss = 0.205    accuracy = 0.875\n",
      "2018-02-25T15:22:43.233795: Epoch   3 Batch  505/1425   train_loss = 0.227    accuracy = 0.938\n",
      "2018-02-25T15:22:54.096544: Epoch   3 Batch  525/1425   train_loss = 0.422    accuracy = 0.750\n",
      "2018-02-25T15:23:04.960109: Epoch   3 Batch  545/1425   train_loss = 0.315    accuracy = 0.938\n",
      "2018-02-25T15:23:15.832505: Epoch   3 Batch  565/1425   train_loss = 0.506    accuracy = 0.750\n",
      "2018-02-25T15:23:26.649087: Epoch   3 Batch  585/1425   train_loss = 0.310    accuracy = 0.938\n",
      "2018-02-25T15:23:37.480667: Epoch   3 Batch  605/1425   train_loss = 0.592    accuracy = 0.688\n",
      "2018-02-25T15:23:48.310838: Epoch   3 Batch  625/1425   train_loss = 0.382    accuracy = 0.812\n",
      "2018-02-25T15:23:59.195271: Epoch   3 Batch  645/1425   train_loss = 0.221    accuracy = 0.938\n",
      "2018-02-25T15:24:10.077876: Epoch   3 Batch  665/1425   train_loss = 0.182    accuracy = 0.938\n",
      "2018-02-25T15:24:20.900422: Epoch   3 Batch  685/1425   train_loss = 0.442    accuracy = 0.750\n",
      "2018-02-25T15:24:31.755804: Epoch   3 Batch  705/1425   train_loss = 0.357    accuracy = 0.812\n",
      "2018-02-25T15:24:42.601270: Epoch   3 Batch  725/1425   train_loss = 0.242    accuracy = 0.812\n",
      "2018-02-25T15:24:53.478709: Epoch   3 Batch  745/1425   train_loss = 0.219    accuracy = 0.938\n",
      "2018-02-25T15:25:04.317553: Epoch   3 Batch  765/1425   train_loss = 0.264    accuracy = 0.812\n",
      "2018-02-25T15:25:15.236816: Epoch   3 Batch  785/1425   train_loss = 0.267    accuracy = 0.938\n",
      "2018-02-25T15:25:26.134507: Epoch   3 Batch  805/1425   train_loss = 0.441    accuracy = 0.688\n",
      "2018-02-25T15:25:37.056955: Epoch   3 Batch  825/1425   train_loss = 0.081    accuracy = 1.000\n",
      "2018-02-25T15:25:47.902778: Epoch   3 Batch  845/1425   train_loss = 0.230    accuracy = 1.000\n",
      "2018-02-25T15:25:58.783491: Epoch   3 Batch  865/1425   train_loss = 0.141    accuracy = 0.938\n",
      "2018-02-25T15:26:09.667299: Epoch   3 Batch  885/1425   train_loss = 0.404    accuracy = 0.812\n",
      "2018-02-25T15:26:20.564780: Epoch   3 Batch  905/1425   train_loss = 0.221    accuracy = 0.938\n",
      "2018-02-25T15:26:31.402716: Epoch   3 Batch  925/1425   train_loss = 0.134    accuracy = 1.000\n",
      "2018-02-25T15:26:42.263962: Epoch   3 Batch  945/1425   train_loss = 0.242    accuracy = 0.938\n",
      "2018-02-25T15:26:53.103912: Epoch   3 Batch  965/1425   train_loss = 0.336    accuracy = 0.875\n",
      "2018-02-25T15:27:03.964637: Epoch   3 Batch  985/1425   train_loss = 0.237    accuracy = 0.875\n",
      "2018-02-25T15:27:14.811760: Epoch   3 Batch 1005/1425   train_loss = 0.345    accuracy = 0.875\n",
      "2018-02-25T15:27:25.700235: Epoch   3 Batch 1025/1425   train_loss = 0.334    accuracy = 0.812\n",
      "2018-02-25T15:27:36.571213: Epoch   3 Batch 1045/1425   train_loss = 0.466    accuracy = 0.750\n",
      "2018-02-25T15:27:47.417045: Epoch   3 Batch 1065/1425   train_loss = 0.436    accuracy = 0.750\n",
      "2018-02-25T15:27:58.289379: Epoch   3 Batch 1085/1425   train_loss = 0.251    accuracy = 0.938\n",
      "2018-02-25T15:28:09.161993: Epoch   3 Batch 1105/1425   train_loss = 0.227    accuracy = 0.938\n",
      "2018-02-25T15:28:20.036419: Epoch   3 Batch 1125/1425   train_loss = 0.399    accuracy = 0.875\n",
      "2018-02-25T15:28:30.907167: Epoch   3 Batch 1145/1425   train_loss = 0.245    accuracy = 0.875\n",
      "2018-02-25T15:28:41.760765: Epoch   3 Batch 1165/1425   train_loss = 0.314    accuracy = 0.875\n",
      "2018-02-25T15:28:52.602917: Epoch   3 Batch 1185/1425   train_loss = 0.209    accuracy = 0.938\n",
      "2018-02-25T15:29:03.451075: Epoch   3 Batch 1205/1425   train_loss = 0.260    accuracy = 0.938\n",
      "2018-02-25T15:29:14.352091: Epoch   3 Batch 1225/1425   train_loss = 0.254    accuracy = 0.875\n",
      "2018-02-25T15:29:25.221962: Epoch   3 Batch 1245/1425   train_loss = 0.602    accuracy = 0.688\n",
      "2018-02-25T15:29:36.104036: Epoch   3 Batch 1265/1425   train_loss = 0.287    accuracy = 0.938\n",
      "2018-02-25T15:29:46.973603: Epoch   3 Batch 1285/1425   train_loss = 0.345    accuracy = 0.875\n",
      "2018-02-25T15:29:57.820237: Epoch   3 Batch 1305/1425   train_loss = 0.673    accuracy = 0.688\n",
      "2018-02-25T15:30:08.706931: Epoch   3 Batch 1325/1425   train_loss = 0.265    accuracy = 0.875\n",
      "2018-02-25T15:30:19.525589: Epoch   3 Batch 1345/1425   train_loss = 0.423    accuracy = 0.875\n",
      "2018-02-25T15:30:30.405246: Epoch   3 Batch 1365/1425   train_loss = 0.390    accuracy = 0.875\n",
      "2018-02-25T15:30:41.325637: Epoch   3 Batch 1385/1425   train_loss = 0.296    accuracy = 0.812\n",
      "2018-02-25T15:30:52.173781: Epoch   3 Batch 1405/1425   train_loss = 0.399    accuracy = 0.812\n",
      "2018-02-25T15:31:05.380389: Epoch   3 Batch   15/75   test_loss = 0.473    accuracy = 0.750\n",
      "2018-02-25T15:31:09.017795: Epoch   3 Batch   35/75   test_loss = 0.235    accuracy = 0.938\n",
      "2018-02-25T15:31:12.674149: Epoch   3 Batch   55/75   test_loss = 0.212    accuracy = 1.000\n",
      "2018-02-25T15:31:16.692386: Epoch   4 Batch    0/1425   train_loss = 0.294    accuracy = 0.875\n",
      "2018-02-25T15:31:27.556241: Epoch   4 Batch   20/1425   train_loss = 0.388    accuracy = 0.688\n",
      "2018-02-25T15:31:38.388474: Epoch   4 Batch   40/1425   train_loss = 0.342    accuracy = 0.812\n",
      "2018-02-25T15:31:49.258109: Epoch   4 Batch   60/1425   train_loss = 0.181    accuracy = 0.875\n",
      "2018-02-25T15:32:00.146006: Epoch   4 Batch   80/1425   train_loss = 0.233    accuracy = 0.812\n",
      "2018-02-25T15:32:11.008387: Epoch   4 Batch  100/1425   train_loss = 0.167    accuracy = 1.000\n",
      "2018-02-25T15:32:21.861084: Epoch   4 Batch  120/1425   train_loss = 0.149    accuracy = 0.938\n",
      "2018-02-25T15:32:32.703323: Epoch   4 Batch  140/1425   train_loss = 0.471    accuracy = 0.750\n",
      "2018-02-25T15:32:43.565406: Epoch   4 Batch  160/1425   train_loss = 0.574    accuracy = 0.688\n",
      "2018-02-25T15:32:54.436430: Epoch   4 Batch  180/1425   train_loss = 0.484    accuracy = 0.812\n",
      "2018-02-25T15:33:05.279089: Epoch   4 Batch  200/1425   train_loss = 0.175    accuracy = 0.938\n",
      "2018-02-25T15:33:16.161046: Epoch   4 Batch  220/1425   train_loss = 0.187    accuracy = 0.938\n",
      "2018-02-25T15:33:27.049302: Epoch   4 Batch  240/1425   train_loss = 0.435    accuracy = 0.812\n",
      "2018-02-25T15:33:37.916069: Epoch   4 Batch  260/1425   train_loss = 0.232    accuracy = 0.875\n",
      "2018-02-25T15:33:48.769390: Epoch   4 Batch  280/1425   train_loss = 0.248    accuracy = 0.938\n",
      "2018-02-25T15:33:59.615988: Epoch   4 Batch  300/1425   train_loss = 0.497    accuracy = 0.688\n",
      "2018-02-25T15:34:10.478390: Epoch   4 Batch  320/1425   train_loss = 0.177    accuracy = 0.875\n",
      "2018-02-25T15:34:21.323982: Epoch   4 Batch  340/1425   train_loss = 0.313    accuracy = 0.875\n",
      "2018-02-25T15:34:32.179534: Epoch   4 Batch  360/1425   train_loss = 0.425    accuracy = 0.812\n",
      "2018-02-25T15:34:43.067465: Epoch   4 Batch  380/1425   train_loss = 0.425    accuracy = 0.625\n",
      "2018-02-25T15:34:53.917009: Epoch   4 Batch  400/1425   train_loss = 0.447    accuracy = 0.812\n",
      "2018-02-25T15:35:04.806603: Epoch   4 Batch  420/1425   train_loss = 0.390    accuracy = 0.812\n",
      "2018-02-25T15:35:15.676135: Epoch   4 Batch  440/1425   train_loss = 0.456    accuracy = 0.875\n",
      "2018-02-25T15:35:26.534698: Epoch   4 Batch  460/1425   train_loss = 0.156    accuracy = 0.938\n",
      "2018-02-25T15:35:37.414753: Epoch   4 Batch  480/1425   train_loss = 0.355    accuracy = 0.812\n",
      "2018-02-25T15:35:48.288994: Epoch   4 Batch  500/1425   train_loss = 0.126    accuracy = 1.000\n",
      "2018-02-25T15:35:59.124199: Epoch   4 Batch  520/1425   train_loss = 0.232    accuracy = 0.938\n",
      "2018-02-25T15:36:09.999127: Epoch   4 Batch  540/1425   train_loss = 0.178    accuracy = 0.938\n",
      "2018-02-25T15:36:20.855423: Epoch   4 Batch  560/1425   train_loss = 0.409    accuracy = 0.688\n",
      "2018-02-25T15:36:31.720069: Epoch   4 Batch  580/1425   train_loss = 0.273    accuracy = 0.938\n",
      "2018-02-25T15:36:42.578381: Epoch   4 Batch  600/1425   train_loss = 0.174    accuracy = 0.938\n",
      "2018-02-25T15:36:53.409141: Epoch   4 Batch  620/1425   train_loss = 0.321    accuracy = 0.812\n",
      "2018-02-25T15:37:05.138153: Epoch   4 Batch  640/1425   train_loss = 0.169    accuracy = 0.938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-25T15:37:16.265544: Epoch   4 Batch  660/1425   train_loss = 0.194    accuracy = 0.938\n",
      "2018-02-25T15:37:27.182146: Epoch   4 Batch  680/1425   train_loss = 0.216    accuracy = 0.938\n",
      "2018-02-25T15:37:38.098566: Epoch   4 Batch  700/1425   train_loss = 0.309    accuracy = 0.875\n",
      "2018-02-25T15:37:48.952641: Epoch   4 Batch  720/1425   train_loss = 0.287    accuracy = 0.812\n",
      "2018-02-25T15:37:59.853728: Epoch   4 Batch  740/1425   train_loss = 0.072    accuracy = 1.000\n",
      "2018-02-25T15:38:10.714654: Epoch   4 Batch  760/1425   train_loss = 0.251    accuracy = 0.875\n",
      "2018-02-25T15:38:21.571452: Epoch   4 Batch  780/1425   train_loss = 0.164    accuracy = 0.938\n",
      "2018-02-25T15:38:32.485127: Epoch   4 Batch  800/1425   train_loss = 0.519    accuracy = 0.812\n",
      "2018-02-25T15:38:43.385287: Epoch   4 Batch  820/1425   train_loss = 0.270    accuracy = 0.875\n",
      "2018-02-25T15:38:54.262550: Epoch   4 Batch  840/1425   train_loss = 0.321    accuracy = 0.812\n",
      "2018-02-25T15:39:05.187877: Epoch   4 Batch  860/1425   train_loss = 0.626    accuracy = 0.812\n",
      "2018-02-25T15:39:16.122679: Epoch   4 Batch  880/1425   train_loss = 0.398    accuracy = 0.875\n",
      "2018-02-25T15:39:27.018720: Epoch   4 Batch  900/1425   train_loss = 0.484    accuracy = 0.875\n",
      "2018-02-25T15:39:37.880354: Epoch   4 Batch  920/1425   train_loss = 0.436    accuracy = 0.875\n",
      "2018-02-25T15:39:48.775536: Epoch   4 Batch  940/1425   train_loss = 0.364    accuracy = 0.750\n",
      "2018-02-25T15:39:59.667186: Epoch   4 Batch  960/1425   train_loss = 0.791    accuracy = 0.750\n",
      "2018-02-25T15:40:10.572721: Epoch   4 Batch  980/1425   train_loss = 0.202    accuracy = 0.875\n",
      "2018-02-25T15:40:21.469670: Epoch   4 Batch 1000/1425   train_loss = 0.332    accuracy = 0.812\n",
      "2018-02-25T15:40:32.364556: Epoch   4 Batch 1020/1425   train_loss = 0.535    accuracy = 0.812\n",
      "2018-02-25T15:40:43.255739: Epoch   4 Batch 1040/1425   train_loss = 0.477    accuracy = 0.750\n",
      "2018-02-25T15:40:54.166611: Epoch   4 Batch 1060/1425   train_loss = 0.215    accuracy = 0.875\n",
      "2018-02-25T15:41:05.070243: Epoch   4 Batch 1080/1425   train_loss = 0.100    accuracy = 1.000\n",
      "2018-02-25T15:41:15.988294: Epoch   4 Batch 1100/1425   train_loss = 0.111    accuracy = 0.938\n",
      "2018-02-25T15:41:26.905854: Epoch   4 Batch 1120/1425   train_loss = 0.263    accuracy = 0.875\n",
      "2018-02-25T15:41:37.776929: Epoch   4 Batch 1140/1425   train_loss = 0.197    accuracy = 0.875\n",
      "2018-02-25T15:41:48.631874: Epoch   4 Batch 1160/1425   train_loss = 0.315    accuracy = 0.938\n",
      "2018-02-25T15:41:59.497362: Epoch   4 Batch 1180/1425   train_loss = 0.378    accuracy = 0.938\n",
      "2018-02-25T15:42:10.387150: Epoch   4 Batch 1200/1425   train_loss = 0.269    accuracy = 0.875\n",
      "2018-02-25T15:42:21.261977: Epoch   4 Batch 1220/1425   train_loss = 0.328    accuracy = 0.875\n",
      "2018-02-25T15:42:32.145753: Epoch   4 Batch 1240/1425   train_loss = 0.599    accuracy = 0.688\n",
      "2018-02-25T15:42:43.058536: Epoch   4 Batch 1260/1425   train_loss = 0.412    accuracy = 0.875\n",
      "2018-02-25T15:42:53.946734: Epoch   4 Batch 1280/1425   train_loss = 0.526    accuracy = 0.812\n",
      "2018-02-25T15:43:04.866619: Epoch   4 Batch 1300/1425   train_loss = 0.101    accuracy = 1.000\n",
      "2018-02-25T15:43:15.772664: Epoch   4 Batch 1320/1425   train_loss = 0.292    accuracy = 0.938\n",
      "2018-02-25T15:43:26.680429: Epoch   4 Batch 1340/1425   train_loss = 0.201    accuracy = 0.875\n",
      "2018-02-25T15:43:37.560472: Epoch   4 Batch 1360/1425   train_loss = 0.201    accuracy = 0.938\n",
      "2018-02-25T15:43:48.426159: Epoch   4 Batch 1380/1425   train_loss = 0.242    accuracy = 0.875\n",
      "2018-02-25T15:43:59.321836: Epoch   4 Batch 1400/1425   train_loss = 0.581    accuracy = 0.625\n",
      "2018-02-25T15:44:10.171460: Epoch   4 Batch 1420/1425   train_loss = 0.186    accuracy = 0.938\n",
      "2018-02-25T15:44:12.537740: Epoch   4 Batch    0/75   test_loss = 0.396    accuracy = 0.688\n",
      "2018-02-25T15:44:16.163201: Epoch   4 Batch   20/75   test_loss = 0.251    accuracy = 0.938\n",
      "2018-02-25T15:44:19.756529: Epoch   4 Batch   40/75   test_loss = 0.221    accuracy = 0.875\n",
      "2018-02-25T15:44:23.381729: Epoch   4 Batch   60/75   test_loss = 0.361    accuracy = 0.750\n",
      "2018-02-25T15:44:34.644431: Epoch   5 Batch   15/1425   train_loss = 0.258    accuracy = 0.938\n",
      "2018-02-25T15:44:45.536554: Epoch   5 Batch   35/1425   train_loss = 0.114    accuracy = 0.938\n",
      "2018-02-25T15:44:56.401826: Epoch   5 Batch   55/1425   train_loss = 0.080    accuracy = 1.000\n",
      "2018-02-25T15:45:07.286533: Epoch   5 Batch   75/1425   train_loss = 0.229    accuracy = 0.875\n",
      "2018-02-25T15:45:18.143500: Epoch   5 Batch   95/1425   train_loss = 0.188    accuracy = 0.875\n",
      "2018-02-25T15:45:29.027454: Epoch   5 Batch  115/1425   train_loss = 0.099    accuracy = 0.938\n",
      "2018-02-25T15:45:39.955135: Epoch   5 Batch  135/1425   train_loss = 0.376    accuracy = 0.750\n",
      "2018-02-25T15:45:50.868220: Epoch   5 Batch  155/1425   train_loss = 0.189    accuracy = 0.875\n",
      "2018-02-25T15:46:01.762598: Epoch   5 Batch  175/1425   train_loss = 0.117    accuracy = 0.938\n",
      "2018-02-25T15:46:12.637925: Epoch   5 Batch  195/1425   train_loss = 0.227    accuracy = 0.875\n",
      "2018-02-25T15:46:23.545451: Epoch   5 Batch  215/1425   train_loss = 0.100    accuracy = 0.938\n",
      "2018-02-25T15:46:34.404627: Epoch   5 Batch  235/1425   train_loss = 0.160    accuracy = 0.938\n",
      "2018-02-25T15:46:45.324192: Epoch   5 Batch  255/1425   train_loss = 0.190    accuracy = 0.938\n",
      "2018-02-25T15:46:56.206026: Epoch   5 Batch  275/1425   train_loss = 0.034    accuracy = 1.000\n",
      "2018-02-25T15:47:07.113647: Epoch   5 Batch  295/1425   train_loss = 0.157    accuracy = 0.938\n",
      "2018-02-25T15:47:18.014091: Epoch   5 Batch  315/1425   train_loss = 0.115    accuracy = 1.000\n",
      "2018-02-25T15:47:28.891871: Epoch   5 Batch  335/1425   train_loss = 0.276    accuracy = 0.875\n",
      "2018-02-25T15:47:39.791341: Epoch   5 Batch  355/1425   train_loss = 0.279    accuracy = 0.875\n",
      "2018-02-25T15:47:50.713038: Epoch   5 Batch  375/1425   train_loss = 0.186    accuracy = 0.938\n",
      "2018-02-25T15:48:01.612174: Epoch   5 Batch  395/1425   train_loss = 0.161    accuracy = 1.000\n",
      "2018-02-25T15:48:12.505332: Epoch   5 Batch  415/1425   train_loss = 0.157    accuracy = 0.938\n",
      "2018-02-25T15:48:23.440777: Epoch   5 Batch  435/1425   train_loss = 0.335    accuracy = 0.875\n",
      "2018-02-25T15:48:34.322673: Epoch   5 Batch  455/1425   train_loss = 0.238    accuracy = 0.938\n",
      "2018-02-25T15:48:45.203997: Epoch   5 Batch  475/1425   train_loss = 0.162    accuracy = 0.938\n",
      "2018-02-25T15:48:56.089146: Epoch   5 Batch  495/1425   train_loss = 0.135    accuracy = 0.938\n",
      "2018-02-25T15:49:06.972212: Epoch   5 Batch  515/1425   train_loss = 0.337    accuracy = 0.875\n",
      "2018-02-25T15:49:17.847882: Epoch   5 Batch  535/1425   train_loss = 0.093    accuracy = 1.000\n",
      "2018-02-25T15:49:28.738624: Epoch   5 Batch  555/1425   train_loss = 0.258    accuracy = 0.875\n",
      "2018-02-25T15:49:39.630282: Epoch   5 Batch  575/1425   train_loss = 0.193    accuracy = 0.938\n",
      "2018-02-25T15:49:50.498621: Epoch   5 Batch  595/1425   train_loss = 0.231    accuracy = 0.875\n",
      "2018-02-25T15:50:01.388122: Epoch   5 Batch  615/1425   train_loss = 0.197    accuracy = 0.875\n",
      "2018-02-25T15:50:12.291017: Epoch   5 Batch  635/1425   train_loss = 0.164    accuracy = 0.938\n",
      "2018-02-25T15:50:23.202998: Epoch   5 Batch  655/1425   train_loss = 0.118    accuracy = 0.938\n",
      "2018-02-25T15:50:34.105933: Epoch   5 Batch  675/1425   train_loss = 0.227    accuracy = 0.875\n",
      "2018-02-25T15:50:44.990466: Epoch   5 Batch  695/1425   train_loss = 0.520    accuracy = 0.812\n",
      "2018-02-25T15:50:55.848187: Epoch   5 Batch  715/1425   train_loss = 0.151    accuracy = 0.938\n",
      "2018-02-25T15:51:06.765106: Epoch   5 Batch  735/1425   train_loss = 0.296    accuracy = 0.938\n",
      "2018-02-25T15:51:17.699026: Epoch   5 Batch  755/1425   train_loss = 0.387    accuracy = 0.875\n",
      "2018-02-25T15:51:28.583017: Epoch   5 Batch  775/1425   train_loss = 0.279    accuracy = 0.812\n",
      "2018-02-25T15:51:39.444555: Epoch   5 Batch  795/1425   train_loss = 0.114    accuracy = 1.000\n",
      "2018-02-25T15:51:50.335311: Epoch   5 Batch  815/1425   train_loss = 0.356    accuracy = 0.875\n",
      "2018-02-25T15:52:01.246859: Epoch   5 Batch  835/1425   train_loss = 0.090    accuracy = 1.000\n",
      "2018-02-25T15:52:12.142313: Epoch   5 Batch  855/1425   train_loss = 0.145    accuracy = 0.875\n",
      "2018-02-25T15:52:23.216442: Epoch   5 Batch  875/1425   train_loss = 0.119    accuracy = 0.938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-25T15:52:34.143235: Epoch   5 Batch  895/1425   train_loss = 0.214    accuracy = 0.938\n",
      "2018-02-25T15:52:45.057210: Epoch   5 Batch  915/1425   train_loss = 0.286    accuracy = 0.875\n",
      "2018-02-25T15:52:56.000239: Epoch   5 Batch  935/1425   train_loss = 0.167    accuracy = 0.938\n",
      "2018-02-25T15:53:06.825157: Epoch   5 Batch  955/1425   train_loss = 0.359    accuracy = 0.875\n",
      "2018-02-25T15:53:17.721890: Epoch   5 Batch  975/1425   train_loss = 0.508    accuracy = 0.812\n",
      "2018-02-25T15:53:28.586222: Epoch   5 Batch  995/1425   train_loss = 0.296    accuracy = 0.875\n",
      "2018-02-25T15:53:39.470804: Epoch   5 Batch 1015/1425   train_loss = 0.158    accuracy = 0.875\n",
      "2018-02-25T15:53:50.332593: Epoch   5 Batch 1035/1425   train_loss = 0.194    accuracy = 0.938\n",
      "2018-02-25T15:54:01.171206: Epoch   5 Batch 1055/1425   train_loss = 0.655    accuracy = 0.562\n",
      "2018-02-25T15:54:12.043031: Epoch   5 Batch 1075/1425   train_loss = 0.186    accuracy = 1.000\n",
      "2018-02-25T15:54:22.857968: Epoch   5 Batch 1095/1425   train_loss = 0.136    accuracy = 0.875\n",
      "2018-02-25T15:54:33.693158: Epoch   5 Batch 1115/1425   train_loss = 0.156    accuracy = 0.938\n",
      "2018-02-25T15:54:44.561892: Epoch   5 Batch 1135/1425   train_loss = 0.258    accuracy = 0.875\n",
      "2018-02-25T15:54:55.391080: Epoch   5 Batch 1155/1425   train_loss = 0.359    accuracy = 0.812\n",
      "2018-02-25T15:55:06.283312: Epoch   5 Batch 1175/1425   train_loss = 0.111    accuracy = 0.938\n",
      "2018-02-25T15:55:17.197012: Epoch   5 Batch 1195/1425   train_loss = 0.250    accuracy = 0.875\n",
      "2018-02-25T15:55:28.081041: Epoch   5 Batch 1215/1425   train_loss = 0.043    accuracy = 1.000\n",
      "2018-02-25T15:55:38.915081: Epoch   5 Batch 1235/1425   train_loss = 0.173    accuracy = 0.938\n",
      "2018-02-25T15:55:49.780494: Epoch   5 Batch 1255/1425   train_loss = 0.275    accuracy = 0.875\n",
      "2018-02-25T15:56:00.636896: Epoch   5 Batch 1275/1425   train_loss = 0.205    accuracy = 0.875\n",
      "2018-02-25T15:56:11.493175: Epoch   5 Batch 1295/1425   train_loss = 0.233    accuracy = 0.875\n",
      "2018-02-25T15:56:22.357769: Epoch   5 Batch 1315/1425   train_loss = 0.222    accuracy = 0.875\n",
      "2018-02-25T15:56:33.260076: Epoch   5 Batch 1335/1425   train_loss = 0.293    accuracy = 0.875\n",
      "2018-02-25T15:56:44.123477: Epoch   5 Batch 1355/1425   train_loss = 0.183    accuracy = 0.875\n",
      "2018-02-25T15:56:55.008067: Epoch   5 Batch 1375/1425   train_loss = 0.089    accuracy = 0.938\n",
      "2018-02-25T15:57:05.902229: Epoch   5 Batch 1395/1425   train_loss = 0.341    accuracy = 0.812\n",
      "2018-02-25T15:57:16.770246: Epoch   5 Batch 1415/1425   train_loss = 0.089    accuracy = 1.000\n",
      "2018-02-25T15:57:22.760382: Epoch   5 Batch    5/75   test_loss = 0.092    accuracy = 0.938\n",
      "2018-02-25T15:57:26.377300: Epoch   5 Batch   25/75   test_loss = 0.070    accuracy = 1.000\n",
      "2018-02-25T15:57:29.957966: Epoch   5 Batch   45/75   test_loss = 0.255    accuracy = 0.938\n",
      "2018-02-25T15:57:33.591427: Epoch   5 Batch   65/75   test_loss = 0.047    accuracy = 1.000\n",
      "2018-02-25T15:57:41.189327: Epoch   6 Batch   10/1425   train_loss = 0.359    accuracy = 0.812\n",
      "2018-02-25T15:57:52.023529: Epoch   6 Batch   30/1425   train_loss = 0.232    accuracy = 0.875\n",
      "2018-02-25T15:58:02.913988: Epoch   6 Batch   50/1425   train_loss = 0.087    accuracy = 1.000\n",
      "2018-02-25T15:58:13.789188: Epoch   6 Batch   70/1425   train_loss = 0.035    accuracy = 1.000\n",
      "2018-02-25T15:58:24.677017: Epoch   6 Batch   90/1425   train_loss = 0.434    accuracy = 0.812\n",
      "2018-02-25T15:58:35.513827: Epoch   6 Batch  110/1425   train_loss = 0.270    accuracy = 0.812\n",
      "2018-02-25T15:58:46.400396: Epoch   6 Batch  130/1425   train_loss = 0.144    accuracy = 0.938\n",
      "2018-02-25T15:58:57.282518: Epoch   6 Batch  150/1425   train_loss = 0.090    accuracy = 0.938\n",
      "2018-02-25T15:59:08.143790: Epoch   6 Batch  170/1425   train_loss = 0.018    accuracy = 1.000\n",
      "2018-02-25T15:59:19.048573: Epoch   6 Batch  190/1425   train_loss = 0.134    accuracy = 0.938\n",
      "2018-02-25T15:59:29.903085: Epoch   6 Batch  210/1425   train_loss = 0.123    accuracy = 0.938\n",
      "2018-02-25T15:59:40.764430: Epoch   6 Batch  230/1425   train_loss = 0.175    accuracy = 0.938\n",
      "2018-02-25T15:59:51.624785: Epoch   6 Batch  250/1425   train_loss = 0.155    accuracy = 0.938\n",
      "2018-02-25T16:00:02.444858: Epoch   6 Batch  270/1425   train_loss = 0.200    accuracy = 0.875\n",
      "2018-02-25T16:00:13.282362: Epoch   6 Batch  290/1425   train_loss = 0.254    accuracy = 0.875\n",
      "2018-02-25T16:00:24.163244: Epoch   6 Batch  310/1425   train_loss = 0.033    accuracy = 1.000\n",
      "2018-02-25T16:00:35.006132: Epoch   6 Batch  330/1425   train_loss = 0.141    accuracy = 0.938\n",
      "2018-02-25T16:00:46.401571: Epoch   6 Batch  350/1425   train_loss = 0.422    accuracy = 0.750\n",
      "2018-02-25T16:00:57.817844: Epoch   6 Batch  370/1425   train_loss = 0.074    accuracy = 1.000\n",
      "2018-02-25T16:01:08.697318: Epoch   6 Batch  390/1425   train_loss = 0.024    accuracy = 1.000\n",
      "2018-02-25T16:01:19.623923: Epoch   6 Batch  410/1425   train_loss = 0.370    accuracy = 0.875\n",
      "2018-02-25T16:01:30.541909: Epoch   6 Batch  430/1425   train_loss = 0.041    accuracy = 1.000\n",
      "2018-02-25T16:01:41.439903: Epoch   6 Batch  450/1425   train_loss = 0.104    accuracy = 1.000\n",
      "2018-02-25T16:01:52.366432: Epoch   6 Batch  470/1425   train_loss = 0.080    accuracy = 0.938\n",
      "2018-02-25T16:02:03.265557: Epoch   6 Batch  490/1425   train_loss = 0.177    accuracy = 0.938\n",
      "2018-02-25T16:02:14.154911: Epoch   6 Batch  510/1425   train_loss = 0.371    accuracy = 0.875\n",
      "2018-02-25T16:02:25.079839: Epoch   6 Batch  530/1425   train_loss = 0.055    accuracy = 1.000\n",
      "2018-02-25T16:02:35.964086: Epoch   6 Batch  550/1425   train_loss = 0.052    accuracy = 1.000\n",
      "2018-02-25T16:02:46.887697: Epoch   6 Batch  570/1425   train_loss = 0.094    accuracy = 0.938\n",
      "2018-02-25T16:02:57.765926: Epoch   6 Batch  590/1425   train_loss = 0.151    accuracy = 0.938\n",
      "2018-02-25T16:03:08.667230: Epoch   6 Batch  610/1425   train_loss = 0.262    accuracy = 0.875\n",
      "2018-02-25T16:03:19.542963: Epoch   6 Batch  630/1425   train_loss = 0.076    accuracy = 0.938\n",
      "2018-02-25T16:03:30.390163: Epoch   6 Batch  650/1425   train_loss = 0.046    accuracy = 1.000\n",
      "2018-02-25T16:03:41.284783: Epoch   6 Batch  670/1425   train_loss = 0.201    accuracy = 0.938\n",
      "2018-02-25T16:03:52.181912: Epoch   6 Batch  690/1425   train_loss = 0.101    accuracy = 1.000\n",
      "2018-02-25T16:04:03.100161: Epoch   6 Batch  710/1425   train_loss = 0.055    accuracy = 0.938\n",
      "2018-02-25T16:04:14.011332: Epoch   6 Batch  730/1425   train_loss = 0.074    accuracy = 1.000\n",
      "2018-02-25T16:04:24.891796: Epoch   6 Batch  750/1425   train_loss = 0.050    accuracy = 1.000\n",
      "2018-02-25T16:04:35.795407: Epoch   6 Batch  770/1425   train_loss = 0.116    accuracy = 1.000\n",
      "2018-02-25T16:04:46.655346: Epoch   6 Batch  790/1425   train_loss = 0.298    accuracy = 0.875\n",
      "2018-02-25T16:04:57.544617: Epoch   6 Batch  810/1425   train_loss = 0.085    accuracy = 0.938\n",
      "2018-02-25T16:05:08.424878: Epoch   6 Batch  830/1425   train_loss = 0.110    accuracy = 1.000\n",
      "2018-02-25T16:05:19.324171: Epoch   6 Batch  850/1425   train_loss = 0.205    accuracy = 0.938\n",
      "2018-02-25T16:05:30.241192: Epoch   6 Batch  870/1425   train_loss = 0.215    accuracy = 0.875\n",
      "2018-02-25T16:05:41.145329: Epoch   6 Batch  890/1425   train_loss = 0.299    accuracy = 0.875\n",
      "2018-02-25T16:05:52.030252: Epoch   6 Batch  910/1425   train_loss = 0.149    accuracy = 0.938\n",
      "2018-02-25T16:06:02.943360: Epoch   6 Batch  930/1425   train_loss = 0.132    accuracy = 1.000\n",
      "2018-02-25T16:06:13.988387: Epoch   6 Batch  950/1425   train_loss = 0.062    accuracy = 1.000\n",
      "2018-02-25T16:06:24.896177: Epoch   6 Batch  970/1425   train_loss = 0.211    accuracy = 0.875\n",
      "2018-02-25T16:06:35.747988: Epoch   6 Batch  990/1425   train_loss = 0.081    accuracy = 1.000\n",
      "2018-02-25T16:06:46.598061: Epoch   6 Batch 1010/1425   train_loss = 0.221    accuracy = 0.938\n",
      "2018-02-25T16:06:57.446564: Epoch   6 Batch 1030/1425   train_loss = 0.209    accuracy = 0.938\n",
      "2018-02-25T16:07:08.289712: Epoch   6 Batch 1050/1425   train_loss = 0.064    accuracy = 0.938\n",
      "2018-02-25T16:07:19.166107: Epoch   6 Batch 1070/1425   train_loss = 0.337    accuracy = 0.875\n",
      "2018-02-25T16:07:30.035233: Epoch   6 Batch 1090/1425   train_loss = 0.166    accuracy = 0.875\n",
      "2018-02-25T16:07:40.885888: Epoch   6 Batch 1110/1425   train_loss = 0.360    accuracy = 0.812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-25T16:07:51.749032: Epoch   6 Batch 1130/1425   train_loss = 0.143    accuracy = 0.875\n",
      "2018-02-25T16:08:02.597709: Epoch   6 Batch 1150/1425   train_loss = 0.130    accuracy = 0.938\n",
      "2018-02-25T16:08:13.441880: Epoch   6 Batch 1170/1425   train_loss = 0.107    accuracy = 0.938\n",
      "2018-02-25T16:08:24.335107: Epoch   6 Batch 1190/1425   train_loss = 0.133    accuracy = 0.938\n",
      "2018-02-25T16:08:35.189523: Epoch   6 Batch 1210/1425   train_loss = 0.231    accuracy = 0.875\n",
      "2018-02-25T16:08:46.009427: Epoch   6 Batch 1230/1425   train_loss = 0.384    accuracy = 0.875\n",
      "2018-02-25T16:08:56.850196: Epoch   6 Batch 1250/1425   train_loss = 0.186    accuracy = 0.938\n",
      "2018-02-25T16:09:07.711341: Epoch   6 Batch 1270/1425   train_loss = 0.167    accuracy = 0.938\n",
      "2018-02-25T16:09:18.582776: Epoch   6 Batch 1290/1425   train_loss = 0.101    accuracy = 0.938\n",
      "2018-02-25T16:09:29.421100: Epoch   6 Batch 1310/1425   train_loss = 0.019    accuracy = 1.000\n",
      "2018-02-25T16:09:40.285853: Epoch   6 Batch 1330/1425   train_loss = 0.161    accuracy = 0.875\n",
      "2018-02-25T16:09:51.177686: Epoch   6 Batch 1350/1425   train_loss = 0.295    accuracy = 0.875\n",
      "2018-02-25T16:10:02.056550: Epoch   6 Batch 1370/1425   train_loss = 0.384    accuracy = 0.875\n",
      "2018-02-25T16:10:12.937038: Epoch   6 Batch 1390/1425   train_loss = 0.014    accuracy = 1.000\n",
      "2018-02-25T16:10:23.789720: Epoch   6 Batch 1410/1425   train_loss = 0.077    accuracy = 1.000\n",
      "2018-02-25T16:10:33.388540: Epoch   6 Batch   10/75   test_loss = 0.101    accuracy = 0.938\n",
      "2018-02-25T16:10:37.024720: Epoch   6 Batch   30/75   test_loss = 0.085    accuracy = 1.000\n",
      "2018-02-25T16:10:40.652980: Epoch   6 Batch   50/75   test_loss = 0.060    accuracy = 1.000\n",
      "2018-02-25T16:10:44.229991: Epoch   6 Batch   70/75   test_loss = 0.049    accuracy = 1.000\n",
      "2018-02-25T16:10:48.241370: Epoch   7 Batch    5/1425   train_loss = 0.243    accuracy = 0.938\n",
      "2018-02-25T16:10:59.070276: Epoch   7 Batch   25/1425   train_loss = 0.079    accuracy = 0.938\n",
      "2018-02-25T16:11:09.983631: Epoch   7 Batch   45/1425   train_loss = 0.053    accuracy = 1.000\n",
      "2018-02-25T16:11:21.374491: Epoch   7 Batch   65/1425   train_loss = 0.075    accuracy = 1.000\n",
      "2018-02-25T16:11:32.801219: Epoch   7 Batch   85/1425   train_loss = 0.042    accuracy = 1.000\n",
      "2018-02-25T16:11:43.700939: Epoch   7 Batch  105/1425   train_loss = 0.069    accuracy = 1.000\n",
      "2018-02-25T16:11:54.607228: Epoch   7 Batch  125/1425   train_loss = 0.014    accuracy = 1.000\n",
      "2018-02-25T16:12:05.484110: Epoch   7 Batch  145/1425   train_loss = 0.154    accuracy = 0.938\n",
      "2018-02-25T16:12:16.380747: Epoch   7 Batch  165/1425   train_loss = 0.048    accuracy = 1.000\n",
      "2018-02-25T16:12:27.281747: Epoch   7 Batch  185/1425   train_loss = 0.025    accuracy = 1.000\n",
      "2018-02-25T16:12:38.158769: Epoch   7 Batch  205/1425   train_loss = 0.412    accuracy = 0.812\n",
      "2018-02-25T16:12:49.046909: Epoch   7 Batch  225/1425   train_loss = 0.007    accuracy = 1.000\n",
      "2018-02-25T16:12:59.874442: Epoch   7 Batch  245/1425   train_loss = 0.186    accuracy = 0.938\n",
      "2018-02-25T16:13:10.768411: Epoch   7 Batch  265/1425   train_loss = 0.128    accuracy = 0.938\n",
      "2018-02-25T16:13:21.664760: Epoch   7 Batch  285/1425   train_loss = 0.076    accuracy = 0.938\n",
      "2018-02-25T16:13:32.558704: Epoch   7 Batch  305/1425   train_loss = 0.058    accuracy = 1.000\n",
      "2018-02-25T16:13:43.415478: Epoch   7 Batch  325/1425   train_loss = 0.050    accuracy = 1.000\n",
      "2018-02-25T16:13:54.283533: Epoch   7 Batch  345/1425   train_loss = 0.157    accuracy = 0.938\n",
      "2018-02-25T16:14:05.200851: Epoch   7 Batch  365/1425   train_loss = 0.056    accuracy = 1.000\n",
      "2018-02-25T16:14:16.101801: Epoch   7 Batch  385/1425   train_loss = 0.028    accuracy = 1.000\n",
      "2018-02-25T16:14:26.986253: Epoch   7 Batch  405/1425   train_loss = 0.082    accuracy = 0.938\n",
      "2018-02-25T16:14:37.872363: Epoch   7 Batch  425/1425   train_loss = 0.236    accuracy = 0.875\n",
      "2018-02-25T16:14:48.760653: Epoch   7 Batch  445/1425   train_loss = 0.195    accuracy = 0.875\n",
      "2018-02-25T16:14:59.667125: Epoch   7 Batch  465/1425   train_loss = 0.150    accuracy = 0.938\n",
      "2018-02-25T16:15:10.554465: Epoch   7 Batch  485/1425   train_loss = 0.301    accuracy = 0.875\n",
      "2018-02-25T16:15:21.453366: Epoch   7 Batch  505/1425   train_loss = 0.346    accuracy = 0.938\n",
      "2018-02-25T16:15:32.347426: Epoch   7 Batch  525/1425   train_loss = 0.043    accuracy = 1.000\n",
      "2018-02-25T16:15:43.234889: Epoch   7 Batch  545/1425   train_loss = 0.210    accuracy = 0.938\n",
      "2018-02-25T16:15:54.127490: Epoch   7 Batch  565/1425   train_loss = 0.064    accuracy = 1.000\n",
      "2018-02-25T16:16:04.992181: Epoch   7 Batch  585/1425   train_loss = 0.096    accuracy = 0.938\n",
      "2018-02-25T16:16:15.848970: Epoch   7 Batch  605/1425   train_loss = 0.057    accuracy = 1.000\n",
      "2018-02-25T16:16:26.728168: Epoch   7 Batch  625/1425   train_loss = 0.050    accuracy = 1.000\n",
      "2018-02-25T16:16:37.602305: Epoch   7 Batch  645/1425   train_loss = 0.054    accuracy = 1.000\n",
      "2018-02-25T16:16:48.508138: Epoch   7 Batch  665/1425   train_loss = 0.017    accuracy = 1.000\n",
      "2018-02-25T16:16:59.406483: Epoch   7 Batch  685/1425   train_loss = 0.021    accuracy = 1.000\n",
      "2018-02-25T16:17:10.275152: Epoch   7 Batch  705/1425   train_loss = 0.065    accuracy = 0.938\n",
      "2018-02-25T16:17:21.134753: Epoch   7 Batch  725/1425   train_loss = 0.211    accuracy = 0.875\n",
      "2018-02-25T16:17:32.014180: Epoch   7 Batch  745/1425   train_loss = 0.143    accuracy = 0.938\n",
      "2018-02-25T16:17:42.869787: Epoch   7 Batch  765/1425   train_loss = 0.166    accuracy = 0.938\n",
      "2018-02-25T16:17:53.768728: Epoch   7 Batch  785/1425   train_loss = 0.114    accuracy = 0.875\n",
      "2018-02-25T16:18:04.630382: Epoch   7 Batch  805/1425   train_loss = 0.046    accuracy = 1.000\n",
      "2018-02-25T16:18:15.497947: Epoch   7 Batch  825/1425   train_loss = 0.133    accuracy = 0.938\n",
      "2018-02-25T16:18:26.381673: Epoch   7 Batch  845/1425   train_loss = 0.022    accuracy = 1.000\n",
      "2018-02-25T16:18:37.273610: Epoch   7 Batch  865/1425   train_loss = 0.237    accuracy = 0.812\n",
      "2018-02-25T16:18:48.154770: Epoch   7 Batch  885/1425   train_loss = 0.062    accuracy = 1.000\n",
      "2018-02-25T16:18:59.032680: Epoch   7 Batch  905/1425   train_loss = 0.056    accuracy = 1.000\n",
      "2018-02-25T16:19:09.915412: Epoch   7 Batch  925/1425   train_loss = 0.236    accuracy = 0.875\n",
      "2018-02-25T16:19:20.818864: Epoch   7 Batch  945/1425   train_loss = 0.062    accuracy = 1.000\n",
      "2018-02-25T16:19:31.934242: Epoch   7 Batch  965/1425   train_loss = 0.134    accuracy = 0.875\n",
      "2018-02-25T16:19:42.797713: Epoch   7 Batch  985/1425   train_loss = 0.156    accuracy = 0.938\n",
      "2018-02-25T16:19:53.648698: Epoch   7 Batch 1005/1425   train_loss = 0.053    accuracy = 0.938\n",
      "2018-02-25T16:20:04.474254: Epoch   7 Batch 1025/1425   train_loss = 0.066    accuracy = 1.000\n",
      "2018-02-25T16:20:15.326346: Epoch   7 Batch 1045/1425   train_loss = 0.295    accuracy = 0.875\n",
      "2018-02-25T16:20:26.147545: Epoch   7 Batch 1065/1425   train_loss = 0.231    accuracy = 0.812\n",
      "2018-02-25T16:20:36.999523: Epoch   7 Batch 1085/1425   train_loss = 0.075    accuracy = 1.000\n",
      "2018-02-25T16:20:47.826644: Epoch   7 Batch 1105/1425   train_loss = 0.088    accuracy = 0.938\n",
      "2018-02-25T16:20:58.679483: Epoch   7 Batch 1125/1425   train_loss = 0.415    accuracy = 0.812\n",
      "2018-02-25T16:21:09.555948: Epoch   7 Batch 1145/1425   train_loss = 0.101    accuracy = 1.000\n",
      "2018-02-25T16:21:20.441606: Epoch   7 Batch 1165/1425   train_loss = 0.110    accuracy = 0.938\n",
      "2018-02-25T16:21:31.304371: Epoch   7 Batch 1185/1425   train_loss = 0.287    accuracy = 0.875\n",
      "2018-02-25T16:21:42.174852: Epoch   7 Batch 1205/1425   train_loss = 0.032    accuracy = 1.000\n",
      "2018-02-25T16:21:53.016647: Epoch   7 Batch 1225/1425   train_loss = 0.394    accuracy = 0.875\n",
      "2018-02-25T16:22:03.882385: Epoch   7 Batch 1245/1425   train_loss = 0.015    accuracy = 1.000\n",
      "2018-02-25T16:22:14.747711: Epoch   7 Batch 1265/1425   train_loss = 0.119    accuracy = 0.938\n",
      "2018-02-25T16:22:25.629914: Epoch   7 Batch 1285/1425   train_loss = 0.184    accuracy = 0.938\n",
      "2018-02-25T16:22:36.605034: Epoch   7 Batch 1305/1425   train_loss = 0.038    accuracy = 1.000\n",
      "2018-02-25T16:22:47.441863: Epoch   7 Batch 1325/1425   train_loss = 0.157    accuracy = 0.938\n",
      "2018-02-25T16:22:58.309568: Epoch   7 Batch 1345/1425   train_loss = 0.038    accuracy = 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-25T16:23:09.190932: Epoch   7 Batch 1365/1425   train_loss = 0.078    accuracy = 0.938\n",
      "2018-02-25T16:23:20.047418: Epoch   7 Batch 1385/1425   train_loss = 0.119    accuracy = 0.938\n",
      "2018-02-25T16:23:30.924536: Epoch   7 Batch 1405/1425   train_loss = 0.081    accuracy = 0.938\n",
      "2018-02-25T16:23:44.131513: Epoch   7 Batch   15/75   test_loss = 0.163    accuracy = 0.938\n",
      "2018-02-25T16:23:47.727517: Epoch   7 Batch   35/75   test_loss = 0.217    accuracy = 0.938\n",
      "2018-02-25T16:23:51.348690: Epoch   7 Batch   55/75   test_loss = 0.174    accuracy = 0.875\n",
      "2018-02-25T16:23:55.332583: Epoch   8 Batch    0/1425   train_loss = 0.076    accuracy = 0.938\n",
      "2018-02-25T16:24:06.193884: Epoch   8 Batch   20/1425   train_loss = 0.023    accuracy = 1.000\n",
      "2018-02-25T16:24:17.050402: Epoch   8 Batch   40/1425   train_loss = 0.017    accuracy = 1.000\n",
      "2018-02-25T16:24:27.927276: Epoch   8 Batch   60/1425   train_loss = 0.160    accuracy = 0.938\n",
      "2018-02-25T16:24:38.804907: Epoch   8 Batch   80/1425   train_loss = 0.047    accuracy = 1.000\n",
      "2018-02-25T16:24:49.662603: Epoch   8 Batch  100/1425   train_loss = 0.010    accuracy = 1.000\n",
      "2018-02-25T16:25:00.597180: Epoch   8 Batch  120/1425   train_loss = 0.019    accuracy = 1.000\n",
      "2018-02-25T16:25:11.489518: Epoch   8 Batch  140/1425   train_loss = 0.085    accuracy = 1.000\n",
      "2018-02-25T16:25:22.352366: Epoch   8 Batch  160/1425   train_loss = 0.007    accuracy = 1.000\n",
      "2018-02-25T16:25:33.179899: Epoch   8 Batch  180/1425   train_loss = 0.124    accuracy = 0.938\n",
      "2018-02-25T16:25:44.049102: Epoch   8 Batch  200/1425   train_loss = 0.014    accuracy = 1.000\n",
      "2018-02-25T16:25:54.956386: Epoch   8 Batch  220/1425   train_loss = 0.009    accuracy = 1.000\n",
      "2018-02-25T16:26:05.849969: Epoch   8 Batch  240/1425   train_loss = 0.132    accuracy = 0.938\n",
      "2018-02-25T16:26:16.693120: Epoch   8 Batch  260/1425   train_loss = 0.094    accuracy = 1.000\n",
      "2018-02-25T16:26:27.526130: Epoch   8 Batch  280/1425   train_loss = 0.328    accuracy = 0.812\n",
      "2018-02-25T16:26:38.356409: Epoch   8 Batch  300/1425   train_loss = 0.048    accuracy = 1.000\n",
      "2018-02-25T16:26:49.174687: Epoch   8 Batch  320/1425   train_loss = 0.042    accuracy = 1.000\n",
      "2018-02-25T16:27:00.013962: Epoch   8 Batch  340/1425   train_loss = 0.126    accuracy = 0.938\n",
      "2018-02-25T16:27:10.875439: Epoch   8 Batch  360/1425   train_loss = 0.139    accuracy = 0.938\n",
      "2018-02-25T16:27:21.777891: Epoch   8 Batch  380/1425   train_loss = 0.019    accuracy = 1.000\n",
      "2018-02-25T16:27:32.662245: Epoch   8 Batch  400/1425   train_loss = 0.201    accuracy = 0.938\n",
      "2018-02-25T16:27:43.506803: Epoch   8 Batch  420/1425   train_loss = 0.098    accuracy = 0.938\n",
      "2018-02-25T16:27:54.355010: Epoch   8 Batch  440/1425   train_loss = 0.131    accuracy = 0.938\n",
      "2018-02-25T16:28:05.197538: Epoch   8 Batch  460/1425   train_loss = 0.019    accuracy = 1.000\n",
      "2018-02-25T16:28:16.121303: Epoch   8 Batch  480/1425   train_loss = 0.045    accuracy = 1.000\n",
      "2018-02-25T16:28:26.994411: Epoch   8 Batch  500/1425   train_loss = 0.236    accuracy = 0.875\n",
      "2018-02-25T16:28:37.867815: Epoch   8 Batch  520/1425   train_loss = 0.118    accuracy = 0.938\n",
      "2018-02-25T16:28:48.735591: Epoch   8 Batch  540/1425   train_loss = 0.022    accuracy = 1.000\n",
      "2018-02-25T16:28:59.599568: Epoch   8 Batch  560/1425   train_loss = 0.052    accuracy = 1.000\n",
      "2018-02-25T16:29:10.465908: Epoch   8 Batch  580/1425   train_loss = 0.094    accuracy = 0.938\n",
      "2018-02-25T16:29:21.331262: Epoch   8 Batch  600/1425   train_loss = 0.062    accuracy = 1.000\n",
      "2018-02-25T16:29:32.227625: Epoch   8 Batch  620/1425   train_loss = 0.043    accuracy = 1.000\n",
      "2018-02-25T16:29:43.084835: Epoch   8 Batch  640/1425   train_loss = 0.034    accuracy = 1.000\n",
      "2018-02-25T16:29:53.972012: Epoch   8 Batch  660/1425   train_loss = 0.062    accuracy = 0.938\n",
      "2018-02-25T16:30:04.848160: Epoch   8 Batch  680/1425   train_loss = 0.051    accuracy = 1.000\n",
      "2018-02-25T16:30:15.715132: Epoch   8 Batch  700/1425   train_loss = 0.024    accuracy = 1.000\n",
      "2018-02-25T16:30:26.575433: Epoch   8 Batch  720/1425   train_loss = 0.009    accuracy = 1.000\n",
      "2018-02-25T16:30:37.415430: Epoch   8 Batch  740/1425   train_loss = 0.018    accuracy = 1.000\n",
      "2018-02-25T16:30:48.259191: Epoch   8 Batch  760/1425   train_loss = 0.046    accuracy = 1.000\n",
      "2018-02-25T16:30:59.110479: Epoch   8 Batch  780/1425   train_loss = 0.475    accuracy = 0.875\n",
      "2018-02-25T16:31:09.978036: Epoch   8 Batch  800/1425   train_loss = 0.034    accuracy = 1.000\n",
      "2018-02-25T16:31:20.852316: Epoch   8 Batch  820/1425   train_loss = 0.126    accuracy = 0.938\n",
      "2018-02-25T16:31:31.693183: Epoch   8 Batch  840/1425   train_loss = 0.090    accuracy = 0.938\n",
      "2018-02-25T16:31:42.527402: Epoch   8 Batch  860/1425   train_loss = 0.104    accuracy = 0.938\n",
      "2018-02-25T16:31:53.387844: Epoch   8 Batch  880/1425   train_loss = 0.010    accuracy = 1.000\n",
      "2018-02-25T16:32:04.227315: Epoch   8 Batch  900/1425   train_loss = 0.018    accuracy = 1.000\n",
      "2018-02-25T16:32:15.085170: Epoch   8 Batch  920/1425   train_loss = 0.037    accuracy = 1.000\n",
      "2018-02-25T16:32:25.953989: Epoch   8 Batch  940/1425   train_loss = 0.149    accuracy = 0.938\n",
      "2018-02-25T16:32:36.812587: Epoch   8 Batch  960/1425   train_loss = 0.146    accuracy = 0.938\n",
      "2018-02-25T16:32:47.633362: Epoch   8 Batch  980/1425   train_loss = 0.197    accuracy = 0.875\n",
      "2018-02-25T16:32:58.497795: Epoch   8 Batch 1000/1425   train_loss = 0.005    accuracy = 1.000\n",
      "2018-02-25T16:33:09.375808: Epoch   8 Batch 1020/1425   train_loss = 0.054    accuracy = 1.000\n",
      "2018-02-25T16:33:20.242191: Epoch   8 Batch 1040/1425   train_loss = 0.069    accuracy = 1.000\n",
      "2018-02-25T16:33:31.094211: Epoch   8 Batch 1060/1425   train_loss = 0.014    accuracy = 1.000\n",
      "2018-02-25T16:33:41.959308: Epoch   8 Batch 1080/1425   train_loss = 0.063    accuracy = 0.938\n",
      "2018-02-25T16:33:52.827610: Epoch   8 Batch 1100/1425   train_loss = 0.012    accuracy = 1.000\n",
      "2018-02-25T16:34:03.692558: Epoch   8 Batch 1120/1425   train_loss = 0.039    accuracy = 1.000\n",
      "2018-02-25T16:34:14.569855: Epoch   8 Batch 1140/1425   train_loss = 0.118    accuracy = 0.938\n",
      "2018-02-25T16:34:25.441335: Epoch   8 Batch 1160/1425   train_loss = 0.164    accuracy = 0.938\n",
      "2018-02-25T16:34:36.273337: Epoch   8 Batch 1180/1425   train_loss = 0.008    accuracy = 1.000\n",
      "2018-02-25T16:34:47.077244: Epoch   8 Batch 1200/1425   train_loss = 0.060    accuracy = 1.000\n",
      "2018-02-25T16:34:57.890078: Epoch   8 Batch 1220/1425   train_loss = 0.170    accuracy = 0.875\n",
      "2018-02-25T16:35:08.740226: Epoch   8 Batch 1240/1425   train_loss = 0.249    accuracy = 0.875\n",
      "2018-02-25T16:35:19.579001: Epoch   8 Batch 1260/1425   train_loss = 0.175    accuracy = 0.875\n",
      "2018-02-25T16:35:30.466800: Epoch   8 Batch 1280/1425   train_loss = 0.126    accuracy = 0.938\n",
      "2018-02-25T16:35:41.310132: Epoch   8 Batch 1300/1425   train_loss = 0.017    accuracy = 1.000\n",
      "2018-02-25T16:35:52.191778: Epoch   8 Batch 1320/1425   train_loss = 0.091    accuracy = 0.938\n",
      "2018-02-25T16:36:03.032816: Epoch   8 Batch 1340/1425   train_loss = 0.044    accuracy = 1.000\n",
      "2018-02-25T16:36:13.915941: Epoch   8 Batch 1360/1425   train_loss = 0.057    accuracy = 1.000\n",
      "2018-02-25T16:36:24.798215: Epoch   8 Batch 1380/1425   train_loss = 0.231    accuracy = 0.875\n",
      "2018-02-25T16:36:35.714108: Epoch   8 Batch 1400/1425   train_loss = 0.228    accuracy = 0.938\n",
      "2018-02-25T16:36:46.609984: Epoch   8 Batch 1420/1425   train_loss = 0.074    accuracy = 0.938\n",
      "2018-02-25T16:36:48.972068: Epoch   8 Batch    0/75   test_loss = 0.022    accuracy = 1.000\n",
      "2018-02-25T16:36:52.593048: Epoch   8 Batch   20/75   test_loss = 0.101    accuracy = 0.938\n",
      "2018-02-25T16:36:56.205151: Epoch   8 Batch   40/75   test_loss = 0.057    accuracy = 1.000\n",
      "2018-02-25T16:36:59.808736: Epoch   8 Batch   60/75   test_loss = 0.146    accuracy = 0.938\n",
      "2018-02-25T16:37:11.449426: Epoch   9 Batch   15/1425   train_loss = 0.143    accuracy = 0.938\n",
      "2018-02-25T16:37:22.952994: Epoch   9 Batch   35/1425   train_loss = 0.112    accuracy = 0.938\n",
      "2018-02-25T16:37:33.873882: Epoch   9 Batch   55/1425   train_loss = 0.028    accuracy = 1.000\n",
      "2018-02-25T16:37:44.789993: Epoch   9 Batch   75/1425   train_loss = 0.024    accuracy = 1.000\n",
      "2018-02-25T16:37:55.646728: Epoch   9 Batch   95/1425   train_loss = 0.147    accuracy = 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-25T16:38:06.540590: Epoch   9 Batch  115/1425   train_loss = 0.106    accuracy = 1.000\n",
      "2018-02-25T16:38:17.418459: Epoch   9 Batch  135/1425   train_loss = 0.017    accuracy = 1.000\n",
      "2018-02-25T16:38:28.294924: Epoch   9 Batch  155/1425   train_loss = 0.086    accuracy = 0.938\n",
      "2018-02-25T16:38:39.169464: Epoch   9 Batch  175/1425   train_loss = 0.059    accuracy = 1.000\n",
      "2018-02-25T16:38:50.031462: Epoch   9 Batch  195/1425   train_loss = 0.015    accuracy = 1.000\n",
      "2018-02-25T16:39:00.965016: Epoch   9 Batch  215/1425   train_loss = 0.104    accuracy = 0.938\n",
      "2018-02-25T16:39:11.880276: Epoch   9 Batch  235/1425   train_loss = 0.093    accuracy = 0.938\n",
      "2018-02-25T16:39:22.771887: Epoch   9 Batch  255/1425   train_loss = 0.019    accuracy = 1.000\n",
      "2018-02-25T16:39:33.670744: Epoch   9 Batch  275/1425   train_loss = 0.115    accuracy = 0.938\n",
      "2018-02-25T16:39:44.572920: Epoch   9 Batch  295/1425   train_loss = 0.346    accuracy = 0.875\n",
      "2018-02-25T16:39:55.427440: Epoch   9 Batch  315/1425   train_loss = 0.068    accuracy = 1.000\n",
      "2018-02-25T16:40:06.334803: Epoch   9 Batch  335/1425   train_loss = 0.075    accuracy = 0.938\n",
      "2018-02-25T16:40:17.212578: Epoch   9 Batch  355/1425   train_loss = 0.176    accuracy = 0.875\n",
      "2018-02-25T16:40:28.144122: Epoch   9 Batch  375/1425   train_loss = 0.185    accuracy = 0.875\n",
      "2018-02-25T16:40:39.167529: Epoch   9 Batch  395/1425   train_loss = 0.022    accuracy = 1.000\n",
      "2018-02-25T16:40:50.055611: Epoch   9 Batch  415/1425   train_loss = 0.019    accuracy = 1.000\n",
      "2018-02-25T16:41:00.874750: Epoch   9 Batch  435/1425   train_loss = 0.016    accuracy = 1.000\n",
      "2018-02-25T16:41:11.727602: Epoch   9 Batch  455/1425   train_loss = 0.057    accuracy = 0.938\n",
      "2018-02-25T16:41:22.576603: Epoch   9 Batch  475/1425   train_loss = 0.022    accuracy = 1.000\n",
      "2018-02-25T16:41:33.446332: Epoch   9 Batch  495/1425   train_loss = 0.033    accuracy = 1.000\n",
      "2018-02-25T16:41:44.329734: Epoch   9 Batch  515/1425   train_loss = 0.032    accuracy = 1.000\n",
      "2018-02-25T16:41:55.211819: Epoch   9 Batch  535/1425   train_loss = 0.002    accuracy = 1.000\n",
      "2018-02-25T16:42:06.080676: Epoch   9 Batch  555/1425   train_loss = 0.051    accuracy = 1.000\n",
      "2018-02-25T16:42:16.943150: Epoch   9 Batch  575/1425   train_loss = 0.048    accuracy = 1.000\n",
      "2018-02-25T16:42:27.788040: Epoch   9 Batch  595/1425   train_loss = 0.012    accuracy = 1.000\n",
      "2018-02-25T16:42:38.613215: Epoch   9 Batch  615/1425   train_loss = 0.418    accuracy = 0.750\n",
      "2018-02-25T16:42:49.492909: Epoch   9 Batch  635/1425   train_loss = 0.163    accuracy = 0.875\n",
      "2018-02-25T16:43:00.371069: Epoch   9 Batch  655/1425   train_loss = 0.337    accuracy = 0.938\n",
      "2018-02-25T16:43:11.234425: Epoch   9 Batch  675/1425   train_loss = 0.019    accuracy = 1.000\n",
      "2018-02-25T16:43:22.085419: Epoch   9 Batch  695/1425   train_loss = 0.006    accuracy = 1.000\n",
      "2018-02-25T16:43:32.924129: Epoch   9 Batch  715/1425   train_loss = 0.043    accuracy = 1.000\n",
      "2018-02-25T16:43:43.790095: Epoch   9 Batch  735/1425   train_loss = 0.013    accuracy = 1.000\n",
      "2018-02-25T16:43:54.648572: Epoch   9 Batch  755/1425   train_loss = 0.031    accuracy = 1.000\n",
      "2018-02-25T16:44:05.535950: Epoch   9 Batch  775/1425   train_loss = 0.025    accuracy = 1.000\n",
      "2018-02-25T16:44:16.407756: Epoch   9 Batch  795/1425   train_loss = 0.092    accuracy = 0.938\n",
      "2018-02-25T16:44:27.285884: Epoch   9 Batch  815/1425   train_loss = 0.044    accuracy = 1.000\n",
      "2018-02-25T16:44:38.183686: Epoch   9 Batch  835/1425   train_loss = 0.061    accuracy = 1.000\n",
      "2018-02-25T16:44:49.037850: Epoch   9 Batch  855/1425   train_loss = 0.114    accuracy = 0.938\n",
      "2018-02-25T16:44:59.884190: Epoch   9 Batch  875/1425   train_loss = 0.047    accuracy = 1.000\n",
      "2018-02-25T16:45:10.698177: Epoch   9 Batch  895/1425   train_loss = 0.004    accuracy = 1.000\n",
      "2018-02-25T16:45:21.567971: Epoch   9 Batch  915/1425   train_loss = 0.060    accuracy = 1.000\n",
      "2018-02-25T16:45:32.408621: Epoch   9 Batch  935/1425   train_loss = 0.009    accuracy = 1.000\n",
      "2018-02-25T16:45:43.269638: Epoch   9 Batch  955/1425   train_loss = 0.118    accuracy = 0.938\n",
      "2018-02-25T16:45:54.124074: Epoch   9 Batch  975/1425   train_loss = 0.037    accuracy = 1.000\n",
      "2018-02-25T16:46:04.968256: Epoch   9 Batch  995/1425   train_loss = 0.013    accuracy = 1.000\n",
      "2018-02-25T16:46:15.801297: Epoch   9 Batch 1015/1425   train_loss = 0.026    accuracy = 1.000\n",
      "2018-02-25T16:46:26.695120: Epoch   9 Batch 1035/1425   train_loss = 0.004    accuracy = 1.000\n",
      "2018-02-25T16:46:37.557434: Epoch   9 Batch 1055/1425   train_loss = 0.023    accuracy = 1.000\n",
      "2018-02-25T16:46:48.438439: Epoch   9 Batch 1075/1425   train_loss = 0.289    accuracy = 0.938\n",
      "2018-02-25T16:46:59.326461: Epoch   9 Batch 1095/1425   train_loss = 0.001    accuracy = 1.000\n",
      "2018-02-25T16:47:10.215605: Epoch   9 Batch 1115/1425   train_loss = 0.036    accuracy = 1.000\n",
      "2018-02-25T16:47:21.137840: Epoch   9 Batch 1135/1425   train_loss = 0.044    accuracy = 1.000\n",
      "2018-02-25T16:47:31.980902: Epoch   9 Batch 1155/1425   train_loss = 0.001    accuracy = 1.000\n",
      "2018-02-25T16:47:42.840035: Epoch   9 Batch 1175/1425   train_loss = 0.028    accuracy = 1.000\n",
      "2018-02-25T16:47:54.437211: Epoch   9 Batch 1195/1425   train_loss = 0.261    accuracy = 0.938\n",
      "2018-02-25T16:48:05.712215: Epoch   9 Batch 1215/1425   train_loss = 0.007    accuracy = 1.000\n",
      "2018-02-25T16:48:16.629101: Epoch   9 Batch 1235/1425   train_loss = 0.024    accuracy = 1.000\n",
      "2018-02-25T16:48:27.554218: Epoch   9 Batch 1255/1425   train_loss = 0.090    accuracy = 0.938\n",
      "2018-02-25T16:48:38.441388: Epoch   9 Batch 1275/1425   train_loss = 0.010    accuracy = 1.000\n",
      "2018-02-25T16:48:49.316062: Epoch   9 Batch 1295/1425   train_loss = 0.232    accuracy = 0.938\n",
      "2018-02-25T16:49:00.196808: Epoch   9 Batch 1315/1425   train_loss = 0.006    accuracy = 1.000\n",
      "2018-02-25T16:49:11.077195: Epoch   9 Batch 1335/1425   train_loss = 0.012    accuracy = 1.000\n",
      "2018-02-25T16:49:21.995611: Epoch   9 Batch 1355/1425   train_loss = 0.048    accuracy = 1.000\n",
      "2018-02-25T16:49:32.830060: Epoch   9 Batch 1375/1425   train_loss = 0.021    accuracy = 1.000\n",
      "2018-02-25T16:49:43.689374: Epoch   9 Batch 1395/1425   train_loss = 0.195    accuracy = 0.938\n",
      "2018-02-25T16:49:54.581975: Epoch   9 Batch 1415/1425   train_loss = 0.102    accuracy = 0.938\n",
      "2018-02-25T16:50:00.571262: Epoch   9 Batch    5/75   test_loss = 0.121    accuracy = 0.938\n",
      "2018-02-25T16:50:04.154925: Epoch   9 Batch   25/75   test_loss = 0.092    accuracy = 0.938\n",
      "2018-02-25T16:50:07.764271: Epoch   9 Batch   45/75   test_loss = 0.010    accuracy = 1.000\n",
      "2018-02-25T16:50:11.394855: Epoch   9 Batch   65/75   test_loss = 0.030    accuracy = 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yudake/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:77: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.770\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "losses = {'train':[], 'cv':[]}\n",
    "accurates = {'train':[], 'cv':[], 'test':[]}\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "    accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "    train_summary_writer = tf.summary.FileWriter(\"runs/train\", sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(EPOCH_NUMS):\n",
    "        train_X, cv_X, train_y, cv_y = train_test_split(train, train_label, test_size = 0.05)\n",
    "        \n",
    "        train_batches = get_batches(train_X, train_y, BATCH_SIZE)\n",
    "        for batch_i in range(len(train_X) // BATCH_SIZE):\n",
    "            x, y = next(train_batches)\n",
    "            img = np.zeros([BATCH_SIZE, IMG_W, IMG_H, 3])\n",
    "            for i in range(BATCH_SIZE):\n",
    "                im = cv2.imread(x[i])\n",
    "                if (im != None):\n",
    "                    im = cv2.resize(im, (IMG_W, IMG_H))\n",
    "                    img[i]=im\n",
    "            \n",
    "            feed = {image: img,\n",
    "                    label: np.reshape(y, [BATCH_SIZE]),\n",
    "                    LearningRate: learning_rate,\n",
    "                    dropout_keep_prob: 0.5}\n",
    "            step, train_loss, summaries, _, accurate = sess.run([global_step, loss, train_summary_op, train_op, accuracy], feed)  #cost\n",
    "            losses['train'].append(train_loss)\n",
    "            accurates['train'].append(accurate)\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "            \n",
    "            if (epoch_i * (len(train_X) // BATCH_SIZE) + batch_i) % 20 == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}    accuracy = {:.3f}'.format(time_str,\n",
    "                                                                                                         epoch_i,\n",
    "                                                                                                         batch_i,\n",
    "                                                                                                         (len(train_X) // BATCH_SIZE),\n",
    "                                                                                                         train_loss,\n",
    "                                                                                                         accurate))\n",
    "        cv_batches = get_batches(cv_X, cv_y, BATCH_SIZE)\n",
    "        for batch_i in range(len(cv_X) // BATCH_SIZE):\n",
    "            x, y = next(cv_batches)\n",
    "            img = np.zeros([BATCH_SIZE, IMG_W, IMG_H, 3])\n",
    "            for i in range(BATCH_SIZE):\n",
    "                im = cv2.imread(x[i])\n",
    "                if (im != None):\n",
    "                    im = cv2.resize(im, (IMG_W, IMG_H))\n",
    "                    img[i]=im\n",
    "            \n",
    "            feed = {image: img,\n",
    "                    label: np.reshape(y, [BATCH_SIZE]),\n",
    "                    LearningRate: learning_rate,\n",
    "                    dropout_keep_prob: 1}\n",
    "            step, cv_loss, summaries, accurate = sess.run([global_step, loss, train_summary_op, accuracy], feed)  #cost\n",
    "            losses['cv'].append(cv_loss)\n",
    "            accurates['cv'].append(accurate)\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "            \n",
    "            if (epoch_i * (len(cv_X) // BATCH_SIZE) + batch_i) % 20 == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   test_loss = {:.3f}    accuracy = {:.3f}'.format(time_str,\n",
    "                                                                                                        epoch_i,\n",
    "                                                                                                        batch_i,\n",
    "                                                                                                        (len(cv_X) // BATCH_SIZE),\n",
    "                                                                                                        cv_loss,\n",
    "                                                                                                        accurate))\n",
    "    test_batches = get_batches(test, test_label, BATCH_SIZE)\n",
    "    for batch_i in range(len(test) // BATCH_SIZE):\n",
    "        x, y = next(test_batches)\n",
    "        img = np.zeros([BATCH_SIZE, IMG_W, IMG_H, 3])\n",
    "        for i in range(BATCH_SIZE):\n",
    "            im = cv2.imread(x[i])\n",
    "            if (im != None):\n",
    "                im = cv2.resize(im, (IMG_W, IMG_H))\n",
    "                img[i]=im\n",
    "        feed = {image: img,\n",
    "                label: np.reshape(y, [BATCH_SIZE]),\n",
    "                LearningRate: learning_rate,\n",
    "                dropout_keep_prob: 1}\n",
    "        accurate = sess.run([accuracy], feed)  #cost\n",
    "        accurates['test'].append(accurate)\n",
    "    print(\"accuracy = {:.3f}\".format(np.mean(accurates['test'])))\n",
    "    saver.save(sess, 'save/save')\n",
    "    print('Model Trained and Saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
